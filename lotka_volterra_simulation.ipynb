{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f4b89",
   "metadata": {},
   "source": [
    "# Lotka-Volterra Predator-Prey Simulation with Reinforcement Learning\n",
    "\n",
    "This notebook implements the classic Lotka-Volterra predator-prey model and explores how reinforcement learning can be used to manage ecosystem dynamics. The simulation models the population dynamics between prey and predator species over time.\n",
    "\n",
    "**Author:** Muller Matej\n",
    "\n",
    "\n",
    "**Faculty:** Faculty of Informatics in Pula  \n",
    "**Course:** Modeling and Simulations  \n",
    "**Mentors:** Robert Šajina, Darko Etinger\n",
    "\n",
    "\n",
    "## Overview\n",
    "- **Lotka-Volterra Model**: Mathematical model describing predator-prey population dynamics\n",
    "- **Reinforcement Learning**: Agent learns to take actions to maintain population stability\n",
    "- **Visualization**: Interactive plots to explore system behavior under different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e40dd4",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "\n",
    "Importing all the necessary libraries for our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5d9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import random\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37988f63",
   "metadata": {},
   "source": [
    "## 2. Defining Lotka-Volterra Model\n",
    "\n",
    "The Lotka-Volterra equations describe the dynamics of two species:\n",
    "- **dx/dt = αx - βxy** (prey equation)\n",
    "- **dy/dt = δxy - γy** (predator equation)\n",
    "\n",
    "Where:\n",
    "- x = prey population\n",
    "- y = predator population\n",
    "- α = prey growth rate\n",
    "- β = predation rate\n",
    "- δ = efficiency of turning prey into predators\n",
    "- γ = predator death rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0e9c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotka-Volterra function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def lotka_volterra(state, t, alpha, beta, delta, gamma):\n",
    "    \"\"\"\n",
    "    Lotka-Volterra differential equations\n",
    "    \n",
    "    Parameters:\n",
    "    - state: [x, y] where x=prey, y=predator\n",
    "    - t: time\n",
    "    - alpha: prey growth rate\n",
    "    - beta: predator effect on prey\n",
    "    - delta: predator benefit from prey\n",
    "    - gamma: predator mortality rate\n",
    "    \n",
    "    Returns:\n",
    "    - [dx/dt, dy/dt]: rate of change for both populations\n",
    "    \"\"\"\n",
    "    x, y = state\n",
    "    dx = alpha * x - beta * x * y  # Prey growth\n",
    "    dy = delta * x * y - gamma * y  # Predator growth\n",
    "    return [dx, dy]\n",
    "\n",
    "print(\"Lotka-Volterra function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c809d7",
   "metadata": {},
   "source": [
    "## 3. Setting Model Parameters and Initial Conditions\n",
    "\n",
    "Defining the ecological parameters and initial population states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e60cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'alpha': 0.1, 'beta': 0.02, 'delta': 0.01, 'gamma': 0.1}\n",
      "Initial state: 40 prey, 9 predators\n",
      "Time range: 0 to 200.0 with 201 points\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'alpha': 0.1,   # prey growth rate\n",
    "    'beta': 0.02,   # predator effect on prey\n",
    "    'delta': 0.01,  # predator benefit from prey\n",
    "    'gamma': 0.1    # predator mortality rate\n",
    "}\n",
    "\n",
    "initial_state = [40, 9]  # initial number of [prey, predators]\n",
    "times = np.linspace(0, 200, 201)  # time points\n",
    "\n",
    "print(f\"Parameters: {parameters}\")\n",
    "print(f\"Initial state: {initial_state[0]} prey, {initial_state[1]} predators\")\n",
    "print(f\"Time range: 0 to {times[-1]} with {len(times)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497ab66e",
   "metadata": {},
   "source": [
    "## 4. Configuring Reinforcement Learning Settings\n",
    "\n",
    "Setting up the RL environment for ecosystem management. The agent will learn to take actions to maintain population stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a9330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL settings initialized successfully!\n",
      "State space: 16 x 16 = 256 states\n",
      "Action space: 3 actions\n",
      "Q-table dimensions: (16, 16, 3)\n",
      "Available actions: ['do_nothing', 'add_prey', 'remove_predators']\n"
     ]
    }
   ],
   "source": [
    "# RL parameters\n",
    "learning_rate = 0.1      # Learning rate\n",
    "discount_factor = 0.9    # Discount factor\n",
    "epsilon = 0.1            # Exploration rate\n",
    "\n",
    "# Discretize state space\n",
    "prey_bins = np.arange(0, 151, 10)\n",
    "predator_bins = np.arange(0, 151, 10)\n",
    "num_prey_states = len(prey_bins)\n",
    "num_predator_states = len(predator_bins)\n",
    "\n",
    "# Action space\n",
    "actions = [\"do_nothing\", \"add_prey\", \"remove_predators\"]\n",
    "num_actions = len(actions)\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((num_prey_states, num_predator_states, num_actions))\n",
    "\n",
    "print(\"RL settings initialized successfully!\")\n",
    "print(f\"State space: {num_prey_states} x {num_predator_states} = {num_prey_states * num_predator_states} states\")\n",
    "print(f\"Action space: {num_actions} actions\")\n",
    "print(f\"Q-table dimensions: {q_table.shape}\")\n",
    "print(f\"Available actions: {actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf412d",
   "metadata": {},
   "source": [
    "## 5. Implementing RL Helper Functions\n",
    "\n",
    "Functions to handle state discretization, action selection, and reward calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17844b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def get_state_indices(prey_pop, predator_pop):\n",
    "    \"\"\"Function to discretize continuous state into bins\"\"\"\n",
    "    prey_index = np.argmin(np.abs(prey_bins - prey_pop))\n",
    "    predator_index = np.argmin(np.abs(predator_bins - predator_pop))\n",
    "    return prey_index, predator_index\n",
    "\n",
    "def choose_action(prey_index, predator_index, epsilon):\n",
    "    \"\"\"Function to choose action (epsilon-greedy)\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        # Explore: choose random action\n",
    "        return random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        # Exploit: choose best action from Q-table\n",
    "        q_values = q_table[prey_index, predator_index, :]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "def calculate_reward(prey_pop, predator_pop):\n",
    "    \"\"\"Function to calculate reward\"\"\"\n",
    "    # Goal: maintain stable populations (both above minimum thresholds)\n",
    "    min_prey = 5\n",
    "    min_predators = 2\n",
    "    \n",
    "    if prey_pop < min_prey or predator_pop < min_predators:\n",
    "        return -100  # Large penalty for extinction\n",
    "    else:\n",
    "        return 1     # Small reward for survival\n",
    "\n",
    "print(\"RL helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ac105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing helper functions...\n",
      "State indices for (40, 9): (np.int64(4), np.int64(1))\n",
      "Random action chosen: do_nothing\n",
      "Reward for (40, 9): 1\n",
      "Reward for extinction scenario (2, 1): -100\n"
     ]
    }
   ],
   "source": [
    "# Test the helper functions\n",
    "print(\"Testing helper functions...\")\n",
    "test_indices = get_state_indices(40, 9)\n",
    "print(f\"State indices for (40, 9): {test_indices}\")\n",
    "test_action = choose_action(test_indices[0], test_indices[1], 0.5)\n",
    "print(f\"Random action chosen: {actions[test_action]}\")\n",
    "test_reward = calculate_reward(40, 9)\n",
    "print(f\"Reward for (40, 9): {test_reward}\")\n",
    "\n",
    "# Test edge cases\n",
    "test_reward_extinction = calculate_reward(2, 1)\n",
    "print(f\"Reward for extinction scenario (2, 1): {test_reward_extinction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd08c0a",
   "metadata": {},
   "source": [
    "## 6. Running Basic Lotka-Volterra Simulation\n",
    "\n",
    "Running the basic simulation without RL intervention to see the natural dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5be5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic Lotka-Volterra simulation...\n",
      "Simulation completed!\n",
      "Final populations: 0.9 prey, 2.1 predators\n",
      "Max prey population: 42.8\n",
      "Max predator population: 21.4\n",
      "Min prey population: 0.6\n",
      "Min predator population: 0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Running basic Lotka-Volterra simulation...\")\n",
    "solution = odeint(lotka_volterra, initial_state, times, args=(\n",
    "    parameters['alpha'], parameters['beta'], \n",
    "    parameters['delta'], parameters['gamma']\n",
    "))\n",
    "\n",
    "print(f\"Simulation completed!\")\n",
    "print(f\"Final populations: {solution[-1][0]:.1f} prey, {solution[-1][1]:.1f} predators\")\n",
    "print(f\"Max prey population: {np.max(solution[:, 0]):.1f}\")\n",
    "print(f\"Max predator population: {np.max(solution[:, 1]):.1f}\")\n",
    "print(f\"Min prey population: {np.min(solution[:, 0]):.1f}\")\n",
    "print(f\"Min predator population: {np.min(solution[:, 1]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f608b2",
   "metadata": {},
   "source": [
    "## 7. Visualizing Population Dynamics\n",
    "\n",
    "Creating comprehensive visualizations of the population dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f75aeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 1. Populations over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(times, solution[:, 0], 'b-', label='Prey', linewidth=2)\n",
    "plt.plot(times, solution[:, 1], 'r-', label='Predator', linewidth=2)\n",
    "plt.title('Lotka-Volterra: Populations over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Phase space\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(solution[:, 0], solution[:, 1], 'darkblue', linewidth=2)\n",
    "plt.plot(initial_state[0], initial_state[1], 'go', markersize=8, label='Start')\n",
    "plt.plot(solution[-1, 0], solution[-1, 1], 'ro', markersize=8, label='End')\n",
    "plt.title('Phase space (x = prey, y = predator)')\n",
    "plt.xlabel('Prey')\n",
    "plt.ylabel('Predator')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636de3b",
   "metadata": {},
   "source": [
    "## 8. Implementing Action Functions\n",
    "\n",
    "Defining how the agent's actions affect the ecosystem populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e31ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action implementation function defined!\n",
      "\n",
      "Testing apply_action function:\n",
      "Test action 0 (do_nothing):\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 8.0]\n",
      "  Result: [25, 8]\n",
      "\n",
      "Test action 1 (add_prey):\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 25.0 → 30.0 prey\n",
      "  Result: [30, 8]\n",
      "\n",
      "Test action 2 (remove_predators):\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 8.0 → 6.0 predators\n",
      "  Result: [25, 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def apply_action(current_state, action_index, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Printing action index:\", action_index)\n",
    "    \"\"\"\n",
    "    Apply the chosen action to modify the population state\n",
    "    \n",
    "    Parameters:\n",
    "    - current_state: [prey, predator] current populations\n",
    "    - action_index: 0=do_nothing, 1=add_prey, 2=remove_predators\n",
    "    - verbose: whether to print action details\n",
    "    \n",
    "    Returns:\n",
    "    - new_state: [prey, predator] after action\n",
    "    \"\"\"\n",
    "    prey, predator = current_state\n",
    "    \n",
    "    if action_index == 0:  # do_nothing\n",
    "        if verbose:\n",
    "            print(f\"    Action: Do nothing - populations stay at [{prey:.1f}, {predator:.1f}]\")\n",
    "        return [prey, predator]\n",
    "    \n",
    "    elif action_index == 1:  # add_prey\n",
    "        new_prey = prey + 5  # Add 5 prey\n",
    "        if verbose:\n",
    "            print(f\"    Action: Add prey - {prey:.1f} → {new_prey:.1f} prey\")\n",
    "        return [new_prey, predator]\n",
    "    \n",
    "    elif action_index == 2:  # remove_predators\n",
    "        new_predator = max(predator - 2, 0)  # Remove 2 predators (don't go below 0)\n",
    "        if verbose:\n",
    "            print(f\"    Action: Remove predators - {predator:.1f} → {new_predator:.1f} predators\")\n",
    "        return [prey, new_predator]\n",
    "    \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"    ERROR: Unknown action {action_index}\")\n",
    "        return current_state\n",
    "\n",
    "print(\"Action implementation function defined!\")\n",
    "\n",
    "# Test the function\n",
    "print(\"\\nTesting apply_action function:\")\n",
    "test_state = [25, 8]\n",
    "for i, action_name in enumerate(actions):\n",
    "    print(f\"Test action {i} ({action_name}):\")\n",
    "    result = apply_action(test_state, i, verbose=True)  # Test with verbose=True\n",
    "    print(f\"  Result: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0aa659",
   "metadata": {},
   "source": [
    "## 9. Implementing Q-Learning Update\n",
    "\n",
    "Creating the Q-table update mechanism using the Q-learning formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bbb89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table update function defined!\n",
      "\n",
      "Testing Q-table update:\n",
      "Before update: 0.0\n",
      "After update: -10.0\n",
      "\n",
      "Testing action 2 (remove_predators) with positive reward:\n",
      "Before update: 0.0\n",
      "After update: 5.0\n"
     ]
    }
   ],
   "source": [
    "def update_q_table(old_state_indices, action, reward, new_state_indices, learning_rate, discount_factor, verbose=False):\n",
    "    \"\"\"\n",
    "    Update Q-table using Q-learning formula:\n",
    "    Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]\n",
    "    \n",
    "    Parameters:\n",
    "    - old_state_indices: (prey_idx, pred_idx) before action\n",
    "    - action: action index that was taken\n",
    "    - reward: reward received\n",
    "    - new_state_indices: (prey_idx, pred_idx) after action\n",
    "    - learning_rate: α (alpha) \n",
    "    - discount_factor: γ (gamma)\n",
    "    - verbose: whether to print update details\n",
    "    \"\"\"\n",
    "    old_prey_idx, old_pred_idx = old_state_indices\n",
    "    new_prey_idx, new_pred_idx = new_state_indices\n",
    "    \n",
    "    # Current Q-value\n",
    "    old_q_value = q_table[old_prey_idx, old_pred_idx, action]\n",
    "    \n",
    "    # Best future Q-value (max over all actions in new state)\n",
    "    max_future_q = np.max(q_table[new_prey_idx, new_pred_idx, :])\n",
    "    \n",
    "    # Q-learning update formula\n",
    "    new_q_value = old_q_value + learning_rate * (reward + discount_factor * max_future_q - old_q_value)\n",
    "    \n",
    "    # Update the Q-table\n",
    "    q_table[old_prey_idx, old_pred_idx, action] = new_q_value\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"    Q-update: Q[{old_state_indices}][{action}]: {old_q_value:.3f} → {new_q_value:.3f}\")\n",
    "        print(f\"    Formula: {old_q_value:.3f} + {learning_rate}*({reward} + {discount_factor}*{max_future_q:.3f} - {old_q_value:.3f})\")\n",
    "\n",
    "print(\"Q-table update function defined!\")\n",
    "\n",
    "# Test with dummy values\n",
    "print(\"\\nTesting Q-table update:\")\n",
    "print(\"Before update:\", q_table[4, 1, 1])\n",
    "update_q_table((4, 1), 1, -100, (3, 1), learning_rate, discount_factor)\n",
    "print(\"After update:\", q_table[4, 1, 1])\n",
    "\n",
    "# Test action 2 with positive reward\n",
    "print(\"\\nTesting action 2 (remove_predators) with positive reward:\")\n",
    "print(\"Before update:\", q_table[5, 2, 2])\n",
    "update_q_table((5, 2), 2, 50, (4, 2), learning_rate, discount_factor)\n",
    "print(\"After update:\", q_table[5, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87fdb4",
   "metadata": {},
   "source": [
    "## 10. Reinforcement Learning Training Loop\n",
    "\n",
    "Implementing a basic reinforcement learning training loop. The agent learns to make decisions that affect population dynamics through trial and error.\n",
    "The training runs for a small number of episodes (5), with each episode consisting of up to 10 steps.\n",
    "At each step, the agent:\n",
    "\n",
    "- Observes the current state (prey and predator populations)\n",
    "- Chooses an action using an epsilon-greedy strategy\n",
    "- Applies the action to update the environment\n",
    "- Receives a reward based on the new state\n",
    "- Updates its Q-table to learn from the experience\n",
    "\n",
    "Rewards are tracked over time to monitor learning progress, with special handling for extinction events (reward of -100) that end episodes early. This represents a typical Q-learning approach where the agent gradually learns which actions lead to better outcomes in different states of the ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7825ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING RL TRAINING ===\n",
      "Training parameters:\n",
      "  Learning rate: 0.1\n",
      "  Discount factor: 0.9\n",
      "  Epsilon (exploration): 0.1\n",
      "\n",
      "--- EPISODE 1 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.000 → 0.100\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*0.000 - 0.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.100 → 0.199\n",
      "    Formula: 0.100 + 0.1*(1 + 0.9*0.100 - 0.100)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.199 → 0.297\n",
      "    Formula: 0.199 + 0.1*(1 + 0.9*0.199 - 0.199)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.297 → 0.394\n",
      "    Formula: 0.297 + 0.1*(1 + 0.9*0.297 - 0.297)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.585 → 0.679\n",
      "    Formula: 0.585 + 0.1*(1 + 0.9*0.585 - 0.585)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.679 → 0.773\n",
      "    Formula: 0.679 + 0.1*(1 + 0.9*0.679 - 0.679)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.773 → 0.865\n",
      "    Formula: 0.773 + 0.1*(1 + 0.9*0.773 - 0.773)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.865 → 0.956\n",
      "    Formula: 0.865 + 0.1*(1 + 0.9*0.865 - 0.865)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 0.956 → 1.047\n",
      "    Formula: 0.956 + 0.1*(1 + 0.9*0.956 - 0.956)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.047 → 1.136\n",
      "    Formula: 1.047 + 0.1*(1 + 0.9*1.047 - 1.047)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.136 → 1.225\n",
      "    Formula: 1.136 + 0.1*(1 + 0.9*1.136 - 1.136)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.225 → 1.313\n",
      "    Formula: 1.225 + 0.1*(1 + 0.9*1.225 - 1.225)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.313 → 1.399\n",
      "    Formula: 1.313 + 0.1*(1 + 0.9*1.313 - 1.313)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.399 → 1.485\n",
      "    Formula: 1.399 + 0.1*(1 + 0.9*1.399 - 1.399)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.485 → 1.571\n",
      "    Formula: 1.485 + 0.1*(1 + 0.9*1.485 - 1.485)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.571 → 1.655\n",
      "    Formula: 1.571 + 0.1*(1 + 0.9*1.571 - 1.571)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.655 → 1.738\n",
      "    Formula: 1.655 + 0.1*(1 + 0.9*1.655 - 1.655)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 0.000 → 0.256\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*1.738 - 0.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "Episode 1 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 2 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.738 → 1.821\n",
      "    Formula: 1.738 + 0.1*(1 + 0.9*1.738 - 1.738)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.821 → 1.903\n",
      "    Formula: 1.821 + 0.1*(1 + 0.9*1.821 - 1.821)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 0.256 → 0.502\n",
      "    Formula: 0.256 + 0.1*(1 + 0.9*1.903 - 0.256)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.903 → 1.984\n",
      "    Formula: 1.903 + 0.1*(1 + 0.9*1.903 - 1.903)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 1.984 → 2.064\n",
      "    Formula: 1.984 + 0.1*(1 + 0.9*1.984 - 1.984)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.064 → 2.143\n",
      "    Formula: 2.064 + 0.1*(1 + 0.9*2.064 - 2.064)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.143 → 2.222\n",
      "    Formula: 2.143 + 0.1*(1 + 0.9*2.143 - 2.143)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.222 → 2.300\n",
      "    Formula: 2.222 + 0.1*(1 + 0.9*2.222 - 2.222)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.300 → 2.377\n",
      "    Formula: 2.300 + 0.1*(1 + 0.9*2.300 - 2.300)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.377 → 2.453\n",
      "    Formula: 2.377 + 0.1*(1 + 0.9*2.377 - 2.377)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.453 → 2.528\n",
      "    Formula: 2.453 + 0.1*(1 + 0.9*2.453 - 2.453)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.528 → 2.603\n",
      "    Formula: 2.528 + 0.1*(1 + 0.9*2.528 - 2.528)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.603 → 2.677\n",
      "    Formula: 2.603 + 0.1*(1 + 0.9*2.603 - 2.603)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.677 → 2.750\n",
      "    Formula: 2.677 + 0.1*(1 + 0.9*2.677 - 2.677)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.750 → 2.823\n",
      "    Formula: 2.750 + 0.1*(1 + 0.9*2.750 - 2.750)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.823 → 2.894\n",
      "    Formula: 2.823 + 0.1*(1 + 0.9*2.823 - 2.823)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.894 → 2.966\n",
      "    Formula: 2.894 + 0.1*(1 + 0.9*2.894 - 2.894)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 2.966 → 3.036\n",
      "    Formula: 2.966 + 0.1*(1 + 0.9*2.966 - 2.966)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.036 → 3.106\n",
      "    Formula: 3.036 + 0.1*(1 + 0.9*3.036 - 3.036)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 0.000 → 0.379\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*3.106 - 0.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "Episode 2 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 3 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.000 → 0.100\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*0.000 - 0.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.100 → 0.199\n",
      "    Formula: 0.100 + 0.1*(1 + 0.9*0.100 - 0.100)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.199 → 0.297\n",
      "    Formula: 0.199 + 0.1*(1 + 0.9*0.199 - 0.199)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 0.000 → 0.127\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*0.297 - 0.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.297 → 0.394\n",
      "    Formula: 0.297 + 0.1*(1 + 0.9*0.297 - 0.297)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 0.127 → 0.267\n",
      "    Formula: 0.127 + 0.1*(1 + 0.9*0.585 - 0.127)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.585 → 0.679\n",
      "    Formula: 0.585 + 0.1*(1 + 0.9*0.585 - 0.585)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.679 → 0.773\n",
      "    Formula: 0.679 + 0.1*(1 + 0.9*0.679 - 0.679)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.773 → 0.865\n",
      "    Formula: 0.773 + 0.1*(1 + 0.9*0.773 - 0.773)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.865 → 0.956\n",
      "    Formula: 0.865 + 0.1*(1 + 0.9*0.865 - 0.865)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 0.956 → 1.047\n",
      "    Formula: 0.956 + 0.1*(1 + 0.9*0.956 - 0.956)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.047 → 1.136\n",
      "    Formula: 1.047 + 0.1*(1 + 0.9*1.047 - 1.047)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.136 → 1.225\n",
      "    Formula: 1.136 + 0.1*(1 + 0.9*1.136 - 1.136)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.225 → 1.313\n",
      "    Formula: 1.225 + 0.1*(1 + 0.9*1.225 - 1.225)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.313 → 1.399\n",
      "    Formula: 1.313 + 0.1*(1 + 0.9*1.313 - 1.313)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.399 → 1.485\n",
      "    Formula: 1.399 + 0.1*(1 + 0.9*1.399 - 1.399)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.485 → 1.571\n",
      "    Formula: 1.485 + 0.1*(1 + 0.9*1.485 - 1.485)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.571 → 1.655\n",
      "    Formula: 1.571 + 0.1*(1 + 0.9*1.571 - 1.571)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "Episode 3 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 4 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.106 → 3.174\n",
      "    Formula: 3.106 + 0.1*(1 + 0.9*3.106 - 3.106)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.174 → 3.243\n",
      "    Formula: 3.174 + 0.1*(1 + 0.9*3.174 - 3.174)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.243 → 3.310\n",
      "    Formula: 3.243 + 0.1*(1 + 0.9*3.243 - 3.243)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.310 → 3.377\n",
      "    Formula: 3.310 + 0.1*(1 + 0.9*3.310 - 3.310)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.377 → 3.443\n",
      "    Formula: 3.377 + 0.1*(1 + 0.9*3.377 - 3.377)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.443 → 3.509\n",
      "    Formula: 3.443 + 0.1*(1 + 0.9*3.443 - 3.443)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 0.502 → 0.868\n",
      "    Formula: 0.502 + 0.1*(1 + 0.9*3.509 - 0.502)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.509 → 3.574\n",
      "    Formula: 3.509 + 0.1*(1 + 0.9*3.509 - 3.509)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.574 → 3.638\n",
      "    Formula: 3.574 + 0.1*(1 + 0.9*3.574 - 3.574)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.638 → 3.702\n",
      "    Formula: 3.638 + 0.1*(1 + 0.9*3.638 - 3.638)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.702 → 3.765\n",
      "    Formula: 3.702 + 0.1*(1 + 0.9*3.702 - 3.702)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.765 → 3.827\n",
      "    Formula: 3.765 + 0.1*(1 + 0.9*3.765 - 3.765)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.827 → 3.889\n",
      "    Formula: 3.827 + 0.1*(1 + 0.9*3.827 - 3.827)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.889 → 3.950\n",
      "    Formula: 3.889 + 0.1*(1 + 0.9*3.889 - 3.889)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 3.950 → 4.010\n",
      "    Formula: 3.950 + 0.1*(1 + 0.9*3.950 - 3.950)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.010 → 4.070\n",
      "    Formula: 4.010 + 0.1*(1 + 0.9*4.010 - 4.010)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.070 → 4.130\n",
      "    Formula: 4.070 + 0.1*(1 + 0.9*4.070 - 4.070)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.130 → 4.188\n",
      "    Formula: 4.130 + 0.1*(1 + 0.9*4.130 - 4.130)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.188 → 4.246\n",
      "    Formula: 4.188 + 0.1*(1 + 0.9*4.188 - 4.188)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.246 → 4.304\n",
      "    Formula: 4.246 + 0.1*(1 + 0.9*4.246 - 4.246)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "Episode 4 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 5 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.304 → 4.361\n",
      "    Formula: 4.304 + 0.1*(1 + 0.9*4.304 - 4.304)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.361 → 4.417\n",
      "    Formula: 4.361 + 0.1*(1 + 0.9*4.361 - 4.361)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.417 → 4.473\n",
      "    Formula: 4.417 + 0.1*(1 + 0.9*4.417 - 4.417)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.473 → 4.528\n",
      "    Formula: 4.473 + 0.1*(1 + 0.9*4.473 - 4.473)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.528 → 4.583\n",
      "    Formula: 4.528 + 0.1*(1 + 0.9*4.528 - 4.528)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.583 → 4.637\n",
      "    Formula: 4.583 + 0.1*(1 + 0.9*4.583 - 4.583)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.637 → 4.691\n",
      "    Formula: 4.637 + 0.1*(1 + 0.9*4.637 - 4.637)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.691 → 4.744\n",
      "    Formula: 4.691 + 0.1*(1 + 0.9*4.691 - 4.691)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 0.868 → 1.308\n",
      "    Formula: 0.868 + 0.1*(1 + 0.9*4.744 - 0.868)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.744 → 4.797\n",
      "    Formula: 4.744 + 0.1*(1 + 0.9*4.744 - 4.744)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.797 → 4.849\n",
      "    Formula: 4.797 + 0.1*(1 + 0.9*4.797 - 4.797)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 0.379 → 0.878\n",
      "    Formula: 0.379 + 0.1*(1 + 0.9*4.849 - 0.379)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.849 → 4.900\n",
      "    Formula: 4.849 + 0.1*(1 + 0.9*4.849 - 4.849)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.900 → 4.951\n",
      "    Formula: 4.900 + 0.1*(1 + 0.9*4.900 - 4.900)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 4.951 → 5.002\n",
      "    Formula: 4.951 + 0.1*(1 + 0.9*4.951 - 4.951)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 5.002 → 5.052\n",
      "    Formula: 5.002 + 0.1*(1 + 0.9*5.002 - 5.002)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 0.878 → 1.345\n",
      "    Formula: 0.878 + 0.1*(1 + 0.9*5.052 - 0.878)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 5.052 → 5.101\n",
      "    Formula: 5.052 + 0.1*(1 + 0.9*5.052 - 5.052)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 5.101 → 5.150\n",
      "    Formula: 5.101 + 0.1*(1 + 0.9*5.101 - 5.101)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 5.150 → 5.199\n",
      "    Formula: 5.150 + 0.1*(1 + 0.9*5.150 - 5.150)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "Episode 5 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 16.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 10 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.655 → 1.738\n",
      "    Formula: 1.655 + 0.1*(1 + 0.9*1.655 - 1.655)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 0.267 → 0.497\n",
      "    Formula: 0.267 + 0.1*(1 + 0.9*1.738 - 0.267)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.738 → 1.821\n",
      "    Formula: 1.738 + 0.1*(1 + 0.9*1.738 - 1.738)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.821 → 1.903\n",
      "    Formula: 1.821 + 0.1*(1 + 0.9*1.821 - 1.821)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.903 → 1.984\n",
      "    Formula: 1.903 + 0.1*(1 + 0.9*1.903 - 1.903)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 1.984 → 2.064\n",
      "    Formula: 1.984 + 0.1*(1 + 0.9*1.984 - 1.984)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.064 → 2.143\n",
      "    Formula: 2.064 + 0.1*(1 + 0.9*2.064 - 2.064)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.143 → 2.222\n",
      "    Formula: 2.143 + 0.1*(1 + 0.9*2.143 - 2.143)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.222 → 2.300\n",
      "    Formula: 2.222 + 0.1*(1 + 0.9*2.222 - 2.222)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.300 → 2.377\n",
      "    Formula: 2.300 + 0.1*(1 + 0.9*2.300 - 2.300)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.377 → 2.453\n",
      "    Formula: 2.377 + 0.1*(1 + 0.9*2.377 - 2.377)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.453 → 2.528\n",
      "    Formula: 2.453 + 0.1*(1 + 0.9*2.453 - 2.453)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.528 → 2.603\n",
      "    Formula: 2.528 + 0.1*(1 + 0.9*2.528 - 2.528)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.603 → 2.677\n",
      "    Formula: 2.603 + 0.1*(1 + 0.9*2.603 - 2.603)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.677 → 2.750\n",
      "    Formula: 2.677 + 0.1*(1 + 0.9*2.677 - 2.677)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.750 → 2.823\n",
      "    Formula: 2.750 + 0.1*(1 + 0.9*2.750 - 2.750)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.823 → 2.894\n",
      "    Formula: 2.823 + 0.1*(1 + 0.9*2.823 - 2.823)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.894 → 2.966\n",
      "    Formula: 2.894 + 0.1*(1 + 0.9*2.894 - 2.894)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 2.966 → 3.036\n",
      "    Formula: 2.966 + 0.1*(1 + 0.9*2.966 - 2.966)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 3.036 → 3.106\n",
      "    Formula: 3.036 + 0.1*(1 + 0.9*3.036 - 3.036)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 10: Reward = 20, Non-zero Q-values = 11\n",
      "\n",
      "Episode 10 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 20 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 0.897 → 1.461\n",
      "    Formula: 0.897 + 0.1*(1 + 0.9*6.151 - 0.897)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 6.151 → 6.190\n",
      "    Formula: 6.151 + 0.1*(1 + 0.9*6.151 - 6.151)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 6.190 → 6.228\n",
      "    Formula: 6.190 + 0.1*(1 + 0.9*6.190 - 6.190)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 1.461 → 1.415\n",
      "    Formula: 1.461 + 0.1*(1 + 0.9*0.000 - 1.461)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 0.000 → 0.100\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*0.000 - 0.000)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 0.100 → 0.190\n",
      "    Formula: 0.100 + 0.1*(1 + 0.9*0.000 - 0.100)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.000 → 0.100\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*0.000 - 0.000)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.100 → 0.199\n",
      "    Formula: 0.100 + 0.1*(1 + 0.9*0.100 - 0.100)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.199 → 0.297\n",
      "    Formula: 0.199 + 0.1*(1 + 0.9*0.199 - 0.199)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.297 → 0.394\n",
      "    Formula: 0.297 + 0.1*(1 + 0.9*0.297 - 0.297)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.585 → 0.679\n",
      "    Formula: 0.585 + 0.1*(1 + 0.9*0.585 - 0.585)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.679 → 0.773\n",
      "    Formula: 0.679 + 0.1*(1 + 0.9*0.679 - 0.679)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.773 → 0.865\n",
      "    Formula: 0.773 + 0.1*(1 + 0.9*0.773 - 0.773)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.865 → 0.956\n",
      "    Formula: 0.865 + 0.1*(1 + 0.9*0.865 - 0.865)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 0.956 → 1.047\n",
      "    Formula: 0.956 + 0.1*(1 + 0.9*0.956 - 0.956)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.047 → 1.136\n",
      "    Formula: 1.047 + 0.1*(1 + 0.9*1.047 - 1.047)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.136 → 1.225\n",
      "    Formula: 1.136 + 0.1*(1 + 0.9*1.136 - 1.136)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.225 → 1.313\n",
      "    Formula: 1.225 + 0.1*(1 + 0.9*1.225 - 1.225)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 20: Reward = 20, Non-zero Q-values = 17\n",
      "\n",
      "Episode 20 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 30 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 1.825 → 1.907\n",
      "    Formula: 1.825 + 0.1*(1 + 0.9*1.825 - 1.825)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 1.907 → 2.418\n",
      "    Formula: 1.907 + 0.1*(1 + 0.9*6.690 - 1.907)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -7.862 → -16.473\n",
      "    Formula: -7.862 + 0.1*(-100 + 0.9*6.690 - -7.862)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 30: Reward = -98, Non-zero Q-values = 20\n",
      "\n",
      "Episode 30 finished. Total reward: -98\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 40 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.764 → 8.776\n",
      "    Formula: 8.764 + 0.1*(1 + 0.9*8.764 - 8.764)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.776 → 8.788\n",
      "    Formula: 8.776 + 0.1*(1 + 0.9*8.776 - 8.776)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.788 → 8.800\n",
      "    Formula: 8.788 + 0.1*(1 + 0.9*8.788 - 8.788)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.800 → 8.812\n",
      "    Formula: 8.800 + 0.1*(1 + 0.9*8.800 - 8.800)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.812 → 8.824\n",
      "    Formula: 8.812 + 0.1*(1 + 0.9*8.812 - 8.812)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.824 → 8.836\n",
      "    Formula: 8.824 + 0.1*(1 + 0.9*8.824 - 8.824)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.836 → 8.848\n",
      "    Formula: 8.836 + 0.1*(1 + 0.9*8.836 - 8.836)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.848 → 8.859\n",
      "    Formula: 8.848 + 0.1*(1 + 0.9*8.848 - 8.848)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.859 → 8.871\n",
      "    Formula: 8.859 + 0.1*(1 + 0.9*8.859 - 8.859)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.871 → 8.882\n",
      "    Formula: 8.871 + 0.1*(1 + 0.9*8.871 - 8.871)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.882 → 8.893\n",
      "    Formula: 8.882 + 0.1*(1 + 0.9*8.882 - 8.882)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.893 → 8.904\n",
      "    Formula: 8.893 + 0.1*(1 + 0.9*8.893 - 8.893)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 3.244 → 3.821\n",
      "    Formula: 3.244 + 0.1*(1 + 0.9*8.904 - 3.244)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 8.904 → 8.915\n",
      "    Formula: 8.904 + 0.1*(1 + 0.9*8.904 - 8.904)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 3.821 → 3.556\n",
      "    Formula: 3.821 + 0.1*(1 + 0.9*0.190 - 3.821)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 0.190 → 0.288\n",
      "    Formula: 0.190 + 0.1*(1 + 0.9*0.190 - 0.190)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 0.288 → 0.477\n",
      "    Formula: 0.288 + 0.1*(1 + 0.9*1.313 - 0.288)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.313 → 1.399\n",
      "    Formula: 1.313 + 0.1*(1 + 0.9*1.313 - 1.313)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.399 → 1.485\n",
      "    Formula: 1.399 + 0.1*(1 + 0.9*1.399 - 1.399)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 1.485 → 1.571\n",
      "    Formula: 1.485 + 0.1*(1 + 0.9*1.485 - 1.485)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 40: Reward = 20, Non-zero Q-values = 21\n",
      "\n",
      "Episode 40 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 50 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.058 → 9.067\n",
      "    Formula: 9.058 + 0.1*(1 + 0.9*9.058 - 9.058)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.067 → 9.076\n",
      "    Formula: 9.067 + 0.1*(1 + 0.9*9.067 - 9.067)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.076 → 9.086\n",
      "    Formula: 9.076 + 0.1*(1 + 0.9*9.076 - 9.076)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.086 → 9.095\n",
      "    Formula: 9.086 + 0.1*(1 + 0.9*9.086 - 9.086)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.095 → 9.104\n",
      "    Formula: 9.095 + 0.1*(1 + 0.9*9.095 - 9.095)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.104 → 9.113\n",
      "    Formula: 9.104 + 0.1*(1 + 0.9*9.104 - 9.104)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.113 → 9.122\n",
      "    Formula: 9.113 + 0.1*(1 + 0.9*9.113 - 9.113)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.122 → 9.130\n",
      "    Formula: 9.122 + 0.1*(1 + 0.9*9.122 - 9.122)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.130 → 9.139\n",
      "    Formula: 9.130 + 0.1*(1 + 0.9*9.130 - 9.130)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.139 → 9.148\n",
      "    Formula: 9.139 + 0.1*(1 + 0.9*9.139 - 9.139)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.148 → 9.156\n",
      "    Formula: 9.148 + 0.1*(1 + 0.9*9.148 - 9.148)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.156 → 9.165\n",
      "    Formula: 9.156 + 0.1*(1 + 0.9*9.156 - 9.156)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.165 → 9.173\n",
      "    Formula: 9.165 + 0.1*(1 + 0.9*9.165 - 9.165)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.173 → 9.181\n",
      "    Formula: 9.173 + 0.1*(1 + 0.9*9.173 - 9.173)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.181 → 9.189\n",
      "    Formula: 9.181 + 0.1*(1 + 0.9*9.181 - 9.181)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.189 → 9.198\n",
      "    Formula: 9.189 + 0.1*(1 + 0.9*9.189 - 9.189)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 3.637 → 4.201\n",
      "    Formula: 3.637 + 0.1*(1 + 0.9*9.198 - 3.637)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.198 → 9.206\n",
      "    Formula: 9.198 + 0.1*(1 + 0.9*9.198 - 9.198)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 25.0 → 30.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 4.201 → 3.975\n",
      "    Formula: 4.201 + 0.1*(1 + 0.9*1.047 - 4.201)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 1.047 → 1.136\n",
      "    Formula: 1.047 + 0.1*(1 + 0.9*1.047 - 1.047)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "Episode 50: Reward = 20, Non-zero Q-values = 22\n",
      "\n",
      "Episode 50 finished. Total reward: 20\n",
      "Final state: 30.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 60 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][2]: -9.688 → -18.215\n",
      "    Formula: -9.688 + 0.1*(-100 + 0.9*5.597 - -9.688)\n",
      "    Step summary: State [10.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 60: Reward = -100, Non-zero Q-values = 23\n",
      "\n",
      "Episode 60 finished. Total reward: -100\n",
      "Final state: 10.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 70 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.789 → 9.791\n",
      "    Formula: 9.789 + 0.1*(1 + 0.9*9.789 - 9.789)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -19.665 → -16.717\n",
      "    Formula: -19.665 + 0.1*(1 + 0.9*9.791 - -19.665)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.791 → 9.793\n",
      "    Formula: 9.791 + 0.1*(1 + 0.9*9.791 - 9.791)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.793 → 9.795\n",
      "    Formula: 9.793 + 0.1*(1 + 0.9*9.793 - 9.793)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.795 → 9.797\n",
      "    Formula: 9.795 + 0.1*(1 + 0.9*9.795 - 9.795)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -16.717 → -24.163\n",
      "    Formula: -16.717 + 0.1*(-100 + 0.9*9.797 - -16.717)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 70: Reward = -95, Non-zero Q-values = 24\n",
      "\n",
      "Episode 70 finished. Total reward: -95\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 80 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.057 → 8.076\n",
      "    Formula: 8.057 + 0.1*(1 + 0.9*8.057 - 8.057)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.076 → 8.095\n",
      "    Formula: 8.076 + 0.1*(1 + 0.9*8.076 - 8.076)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.095 → 8.114\n",
      "    Formula: 8.095 + 0.1*(1 + 0.9*8.095 - 8.095)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.114 → 8.133\n",
      "    Formula: 8.114 + 0.1*(1 + 0.9*8.114 - 8.114)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.133 → 8.152\n",
      "    Formula: 8.133 + 0.1*(1 + 0.9*8.133 - 8.133)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.152 → 8.170\n",
      "    Formula: 8.152 + 0.1*(1 + 0.9*8.152 - 8.152)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.170 → 8.189\n",
      "    Formula: 8.170 + 0.1*(1 + 0.9*8.170 - 8.170)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.189 → 8.207\n",
      "    Formula: 8.189 + 0.1*(1 + 0.9*8.189 - 8.189)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.207 → 8.225\n",
      "    Formula: 8.207 + 0.1*(1 + 0.9*8.207 - 8.207)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.225 → 8.243\n",
      "    Formula: 8.225 + 0.1*(1 + 0.9*8.225 - 8.225)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.243 → 8.260\n",
      "    Formula: 8.243 + 0.1*(1 + 0.9*8.243 - 8.243)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.260 → 8.278\n",
      "    Formula: 8.260 + 0.1*(1 + 0.9*8.260 - 8.260)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.278 → 8.295\n",
      "    Formula: 8.278 + 0.1*(1 + 0.9*8.278 - 8.278)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.295 → 8.312\n",
      "    Formula: 8.295 + 0.1*(1 + 0.9*8.295 - 8.295)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.312 → 8.329\n",
      "    Formula: 8.312 + 0.1*(1 + 0.9*8.312 - 8.312)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.329 → 8.345\n",
      "    Formula: 8.329 + 0.1*(1 + 0.9*8.329 - 8.329)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.345 → 8.362\n",
      "    Formula: 8.345 + 0.1*(1 + 0.9*8.345 - 8.345)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.362 → 8.378\n",
      "    Formula: 8.362 + 0.1*(1 + 0.9*8.362 - 8.362)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.378 → 8.395\n",
      "    Formula: 8.378 + 0.1*(1 + 0.9*8.378 - 8.378)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 8.395 → 8.411\n",
      "    Formula: 8.395 + 0.1*(1 + 0.9*8.395 - 8.395)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 80: Reward = 20, Non-zero Q-values = 25\n",
      "\n",
      "Episode 80 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 90 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.815 → 9.817\n",
      "    Formula: 9.815 + 0.1*(1 + 0.9*9.815 - 9.815)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.817 → 9.819\n",
      "    Formula: 9.817 + 0.1*(1 + 0.9*9.817 - 9.817)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.819 → 9.820\n",
      "    Formula: 9.819 + 0.1*(1 + 0.9*9.819 - 9.819)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.820 → 9.822\n",
      "    Formula: 9.820 + 0.1*(1 + 0.9*9.820 - 9.820)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.822 → 9.824\n",
      "    Formula: 9.822 + 0.1*(1 + 0.9*9.822 - 9.822)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.824 → 9.826\n",
      "    Formula: 9.824 + 0.1*(1 + 0.9*9.824 - 9.824)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.826 → 9.828\n",
      "    Formula: 9.826 + 0.1*(1 + 0.9*9.826 - 9.826)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.828 → 9.829\n",
      "    Formula: 9.828 + 0.1*(1 + 0.9*9.828 - 9.828)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.829 → 9.831\n",
      "    Formula: 9.829 + 0.1*(1 + 0.9*9.829 - 9.829)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.831 → 9.833\n",
      "    Formula: 9.831 + 0.1*(1 + 0.9*9.831 - 9.831)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.833 → 9.834\n",
      "    Formula: 9.833 + 0.1*(1 + 0.9*9.833 - 9.833)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.834 → 9.836\n",
      "    Formula: 9.834 + 0.1*(1 + 0.9*9.834 - 9.834)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.836 → 9.838\n",
      "    Formula: 9.836 + 0.1*(1 + 0.9*9.836 - 9.836)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.838 → 9.839\n",
      "    Formula: 9.838 + 0.1*(1 + 0.9*9.838 - 9.838)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.839 → 9.841\n",
      "    Formula: 9.839 + 0.1*(1 + 0.9*9.839 - 9.839)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.841 → 9.842\n",
      "    Formula: 9.841 + 0.1*(1 + 0.9*9.841 - 9.841)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.842 → 9.844\n",
      "    Formula: 9.842 + 0.1*(1 + 0.9*9.842 - 9.842)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.844 → 9.846\n",
      "    Formula: 9.844 + 0.1*(1 + 0.9*9.844 - 9.844)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.846 → 9.847\n",
      "    Formula: 9.846 + 0.1*(1 + 0.9*9.846 - 9.846)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.847 → 9.849\n",
      "    Formula: 9.847 + 0.1*(1 + 0.9*9.847 - 9.847)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 90: Reward = 20, Non-zero Q-values = 25\n",
      "\n",
      "Episode 90 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 100 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -40.710 → -35.644\n",
      "    Formula: -40.710 + 0.1*(1 + 0.9*9.950 - -40.710)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.950 → 9.950\n",
      "    Formula: 9.950 + 0.1*(1 + 0.9*9.950 - 9.950)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.950 → 9.951\n",
      "    Formula: 9.950 + 0.1*(1 + 0.9*9.950 - 9.950)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.951 → 9.951\n",
      "    Formula: 9.951 + 0.1*(1 + 0.9*9.951 - 9.951)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.951 → 9.952\n",
      "    Formula: 9.951 + 0.1*(1 + 0.9*9.951 - 9.951)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.952 → 9.952\n",
      "    Formula: 9.952 + 0.1*(1 + 0.9*9.952 - 9.952)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.952 → 9.953\n",
      "    Formula: 9.952 + 0.1*(1 + 0.9*9.952 - 9.952)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.953 → 9.953\n",
      "    Formula: 9.953 + 0.1*(1 + 0.9*9.953 - 9.953)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.953 → 9.954\n",
      "    Formula: 9.953 + 0.1*(1 + 0.9*9.953 - 9.953)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.954 → 9.954\n",
      "    Formula: 9.954 + 0.1*(1 + 0.9*9.954 - 9.954)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.954 → 9.955\n",
      "    Formula: 9.954 + 0.1*(1 + 0.9*9.954 - 9.954)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.955 → 9.955\n",
      "    Formula: 9.955 + 0.1*(1 + 0.9*9.955 - 9.955)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.955 → 9.956\n",
      "    Formula: 9.955 + 0.1*(1 + 0.9*9.955 - 9.955)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.956 → 9.956\n",
      "    Formula: 9.956 + 0.1*(1 + 0.9*9.956 - 9.956)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.956 → 9.956\n",
      "    Formula: 9.956 + 0.1*(1 + 0.9*9.956 - 9.956)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.956 → 9.957\n",
      "    Formula: 9.956 + 0.1*(1 + 0.9*9.956 - 9.956)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.957 → 9.957\n",
      "    Formula: 9.957 + 0.1*(1 + 0.9*9.957 - 9.957)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.957 → 9.958\n",
      "    Formula: 9.957 + 0.1*(1 + 0.9*9.957 - 9.957)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.958 → 9.958\n",
      "    Formula: 9.958 + 0.1*(1 + 0.9*9.958 - 9.958)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.958 → 9.959\n",
      "    Formula: 9.958 + 0.1*(1 + 0.9*9.958 - 9.958)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 100: Reward = 20, Non-zero Q-values = 26\n",
      "\n",
      "Episode 100 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 110 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.813 → 9.815\n",
      "    Formula: 9.813 + 0.1*(1 + 0.9*9.813 - 9.813)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.815 → 9.817\n",
      "    Formula: 9.815 + 0.1*(1 + 0.9*9.815 - 9.815)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.817 → 9.819\n",
      "    Formula: 9.817 + 0.1*(1 + 0.9*9.817 - 9.817)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.819 → 9.820\n",
      "    Formula: 9.819 + 0.1*(1 + 0.9*9.819 - 9.819)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.820 → 9.822\n",
      "    Formula: 9.820 + 0.1*(1 + 0.9*9.820 - 9.820)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.822 → 9.824\n",
      "    Formula: 9.822 + 0.1*(1 + 0.9*9.822 - 9.822)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.824 → 9.826\n",
      "    Formula: 9.824 + 0.1*(1 + 0.9*9.824 - 9.824)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.826 → 9.828\n",
      "    Formula: 9.826 + 0.1*(1 + 0.9*9.826 - 9.826)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.828 → 9.829\n",
      "    Formula: 9.828 + 0.1*(1 + 0.9*9.828 - 9.828)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.829 → 9.831\n",
      "    Formula: 9.829 + 0.1*(1 + 0.9*9.829 - 9.829)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.831 → 9.833\n",
      "    Formula: 9.831 + 0.1*(1 + 0.9*9.831 - 9.831)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.833 → 9.834\n",
      "    Formula: 9.833 + 0.1*(1 + 0.9*9.833 - 9.833)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.834 → 9.836\n",
      "    Formula: 9.834 + 0.1*(1 + 0.9*9.834 - 9.834)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.836 → 9.838\n",
      "    Formula: 9.836 + 0.1*(1 + 0.9*9.836 - 9.836)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.838 → 9.839\n",
      "    Formula: 9.838 + 0.1*(1 + 0.9*9.838 - 9.838)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.839 → 9.841\n",
      "    Formula: 9.839 + 0.1*(1 + 0.9*9.839 - 9.839)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.841 → 9.842\n",
      "    Formula: 9.841 + 0.1*(1 + 0.9*9.841 - 9.841)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.842 → 9.844\n",
      "    Formula: 9.842 + 0.1*(1 + 0.9*9.842 - 9.842)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.844 → 9.846\n",
      "    Formula: 9.844 + 0.1*(1 + 0.9*9.844 - 9.844)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.846 → 9.847\n",
      "    Formula: 9.846 + 0.1*(1 + 0.9*9.846 - 9.846)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "Episode 110: Reward = 20, Non-zero Q-values = 28\n",
      "\n",
      "Episode 110 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 120 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 7.990 → 8.010\n",
      "    Formula: 7.990 + 0.1*(1 + 0.9*7.990 - 7.990)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 8.010 → 8.205\n",
      "    Formula: 8.010 + 0.1*(1 + 0.9*9.964 - 8.010)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -46.168 → -50.654\n",
      "    Formula: -46.168 + 0.1*(-100 + 0.9*9.964 - -46.168)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 120: Reward = -98, Non-zero Q-values = 28\n",
      "\n",
      "Episode 120 finished. Total reward: -98\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 130 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.978 → 9.978\n",
      "    Formula: 9.978 + 0.1*(1 + 0.9*9.978 - 9.978)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.978 → 9.979\n",
      "    Formula: 9.978 + 0.1*(1 + 0.9*9.978 - 9.978)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.979 → 9.979\n",
      "    Formula: 9.979 + 0.1*(1 + 0.9*9.979 - 9.979)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.979 → 9.979\n",
      "    Formula: 9.979 + 0.1*(1 + 0.9*9.979 - 9.979)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.979 → 9.979\n",
      "    Formula: 9.979 + 0.1*(1 + 0.9*9.979 - 9.979)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.979 → 9.980\n",
      "    Formula: 9.979 + 0.1*(1 + 0.9*9.979 - 9.979)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.980 → 9.980\n",
      "    Formula: 9.980 + 0.1*(1 + 0.9*9.980 - 9.980)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.980 → 9.980\n",
      "    Formula: 9.980 + 0.1*(1 + 0.9*9.980 - 9.980)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.980 → 9.980\n",
      "    Formula: 9.980 + 0.1*(1 + 0.9*9.980 - 9.980)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.980 → 9.980\n",
      "    Formula: 9.980 + 0.1*(1 + 0.9*9.980 - 9.980)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.980 → 9.981\n",
      "    Formula: 9.980 + 0.1*(1 + 0.9*9.980 - 9.980)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.981\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.981\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.981\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.981\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.981\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.981 → 9.982\n",
      "    Formula: 9.981 + 0.1*(1 + 0.9*9.981 - 9.981)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -49.235 → -43.313\n",
      "    Formula: -49.235 + 0.1*(1 + 0.9*9.982 - -49.235)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 130: Reward = 20, Non-zero Q-values = 29\n",
      "\n",
      "Episode 130 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 140 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 7.773 → 7.994\n",
      "    Formula: 7.773 + 0.1*(1 + 0.9*9.982 - 7.773)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.982 → 9.982\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.982 → 9.983\n",
      "    Formula: 9.982 + 0.1*(1 + 0.9*9.982 - 9.982)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.983\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.983\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 6.229 → 6.604\n",
      "    Formula: 6.229 + 0.1*(1 + 0.9*9.983 - 6.229)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.983\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.983\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.983\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 9.983 → 9.984\n",
      "    Formula: 9.983 + 0.1*(1 + 0.9*9.983 - 9.983)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 6.604 → 6.162\n",
      "    Formula: 6.604 + 0.1*(1 + 0.9*1.313 - 6.604)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 1.313 → 1.399\n",
      "    Formula: 1.313 + 0.1*(1 + 0.9*1.313 - 1.313)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 1.399 → 1.485\n",
      "    Formula: 1.399 + 0.1*(1 + 0.9*1.399 - 1.399)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 1.485 → 1.571\n",
      "    Formula: 1.485 + 0.1*(1 + 0.9*1.485 - 1.485)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][2]: 0.000 → 0.241\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*1.571 - 0.000)\n",
      "    Step summary: State [90.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 1.571 → 1.655\n",
      "    Formula: 1.571 + 0.1*(1 + 0.9*1.571 - 1.571)\n",
      "    Step summary: State [90.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 1.655 → 1.738\n",
      "    Formula: 1.655 + 0.1*(1 + 0.9*1.655 - 1.655)\n",
      "    Step summary: State [90.0, 16.0], Step reward: 1\n",
      "Episode 140: Reward = 20, Non-zero Q-values = 31\n",
      "\n",
      "Episode 140 finished. Total reward: 20\n",
      "Final state: 90.0 prey, 16.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 150 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.934 → 9.934\n",
      "    Formula: 9.934 + 0.1*(1 + 0.9*9.934 - 9.934)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.934 → 9.935\n",
      "    Formula: 9.934 + 0.1*(1 + 0.9*9.934 - 9.934)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.935 → 9.936\n",
      "    Formula: 9.935 + 0.1*(1 + 0.9*9.935 - 9.935)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.936 → 9.936\n",
      "    Formula: 9.936 + 0.1*(1 + 0.9*9.936 - 9.936)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.936 → 9.937\n",
      "    Formula: 9.936 + 0.1*(1 + 0.9*9.936 - 9.936)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.937 → 9.938\n",
      "    Formula: 9.937 + 0.1*(1 + 0.9*9.937 - 9.937)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 7.839 → 8.049\n",
      "    Formula: 7.839 + 0.1*(1 + 0.9*9.938 - 7.839)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.938 → 9.938\n",
      "    Formula: 9.938 + 0.1*(1 + 0.9*9.938 - 9.938)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.938 → 9.939\n",
      "    Formula: 9.938 + 0.1*(1 + 0.9*9.938 - 9.938)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 8.049 → 8.239\n",
      "    Formula: 8.049 + 0.1*(1 + 0.9*9.939 - 8.049)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.939 → 9.939\n",
      "    Formula: 9.939 + 0.1*(1 + 0.9*9.939 - 9.939)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.939 → 9.940\n",
      "    Formula: 9.939 + 0.1*(1 + 0.9*9.939 - 9.939)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.940 → 9.941\n",
      "    Formula: 9.940 + 0.1*(1 + 0.9*9.940 - 9.940)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.941 → 9.941\n",
      "    Formula: 9.941 + 0.1*(1 + 0.9*9.941 - 9.941)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.941 → 9.942\n",
      "    Formula: 9.941 + 0.1*(1 + 0.9*9.941 - 9.941)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.942 → 9.942\n",
      "    Formula: 9.942 + 0.1*(1 + 0.9*9.942 - 9.942)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.942 → 9.943\n",
      "    Formula: 9.942 + 0.1*(1 + 0.9*9.942 - 9.942)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.943 → 9.943\n",
      "    Formula: 9.943 + 0.1*(1 + 0.9*9.943 - 9.943)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.943 → 9.944\n",
      "    Formula: 9.943 + 0.1*(1 + 0.9*9.943 - 9.943)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.944 → 9.945\n",
      "    Formula: 9.944 + 0.1*(1 + 0.9*9.944 - 9.944)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "Episode 150: Reward = 20, Non-zero Q-values = 31\n",
      "\n",
      "Episode 150 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 160 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.997 → 9.998\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 160: Reward = 20, Non-zero Q-values = 31\n",
      "\n",
      "Episode 160 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 170 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.962 → 9.962\n",
      "    Formula: 9.962 + 0.1*(1 + 0.9*9.962 - 9.962)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 6.813 → 7.128\n",
      "    Formula: 6.813 + 0.1*(1 + 0.9*9.962 - 6.813)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.962 → 9.963\n",
      "    Formula: 9.962 + 0.1*(1 + 0.9*9.962 - 9.962)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.963 → 9.963\n",
      "    Formula: 9.963 + 0.1*(1 + 0.9*9.963 - 9.963)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 6.888 → 7.195\n",
      "    Formula: 6.888 + 0.1*(1 + 0.9*9.963 - 6.888)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.963 → 9.963\n",
      "    Formula: 9.963 + 0.1*(1 + 0.9*9.963 - 9.963)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.963 → 9.964\n",
      "    Formula: 9.963 + 0.1*(1 + 0.9*9.963 - 9.963)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.964 → 9.964\n",
      "    Formula: 9.964 + 0.1*(1 + 0.9*9.964 - 9.964)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.964 → 9.964\n",
      "    Formula: 9.964 + 0.1*(1 + 0.9*9.964 - 9.964)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.964 → 9.965\n",
      "    Formula: 9.964 + 0.1*(1 + 0.9*9.964 - 9.964)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.965 → 9.965\n",
      "    Formula: 9.965 + 0.1*(1 + 0.9*9.965 - 9.965)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.965 → 9.965\n",
      "    Formula: 9.965 + 0.1*(1 + 0.9*9.965 - 9.965)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.965 → 9.966\n",
      "    Formula: 9.965 + 0.1*(1 + 0.9*9.965 - 9.965)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.966 → 9.966\n",
      "    Formula: 9.966 + 0.1*(1 + 0.9*9.966 - 9.966)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.966 → 9.966\n",
      "    Formula: 9.966 + 0.1*(1 + 0.9*9.966 - 9.966)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.966 → 9.967\n",
      "    Formula: 9.966 + 0.1*(1 + 0.9*9.966 - 9.966)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.967 → 9.967\n",
      "    Formula: 9.967 + 0.1*(1 + 0.9*9.967 - 9.967)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.128 → 6.551\n",
      "    Formula: 7.128 + 0.1*(1 + 0.9*0.394 - 7.128)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "Episode 170: Reward = 20, Non-zero Q-values = 31\n",
      "\n",
      "Episode 170 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 180 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -45.332 → -39.799\n",
      "    Formula: -45.332 + 0.1*(1 + 0.9*9.999 - -45.332)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -39.799 → -44.919\n",
      "    Formula: -39.799 + 0.1*(-100 + 0.9*9.999 - -39.799)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 180: Reward = -91, Non-zero Q-values = 31\n",
      "\n",
      "Episode 180 finished. Total reward: -91\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 190 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.990\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.990\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.990\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.990\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.990\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.990 → 9.991\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*9.990 - 9.990)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 7.653 → 7.887\n",
      "    Formula: 7.653 + 0.1*(1 + 0.9*9.991 - 7.653)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.053 → 9.147\n",
      "    Formula: 9.053 + 0.1*(1 + 0.9*9.991 - 9.053)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.991\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.991 → 9.992\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*9.991 - 9.991)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.992 → 9.992\n",
      "    Formula: 9.992 + 0.1*(1 + 0.9*9.992 - 9.992)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "Episode 190: Reward = 20, Non-zero Q-values = 31\n",
      "\n",
      "Episode 190 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 200 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.987 → 9.987\n",
      "    Formula: 9.987 + 0.1*(1 + 0.9*9.987 - 9.987)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.987 → 9.988\n",
      "    Formula: 9.987 + 0.1*(1 + 0.9*9.987 - 9.987)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.988\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.988 → 9.989\n",
      "    Formula: 9.988 + 0.1*(1 + 0.9*9.988 - 9.988)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 6.356 → 6.720\n",
      "    Formula: 6.356 + 0.1*(1 + 0.9*9.989 - 6.356)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.989\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.989 → 9.990\n",
      "    Formula: 9.989 + 0.1*(1 + 0.9*9.989 - 9.989)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 200: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 200 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 210 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.993 → 9.993\n",
      "    Formula: 9.993 + 0.1*(1 + 0.9*9.993 - 9.993)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.993 → 9.993\n",
      "    Formula: 9.993 + 0.1*(1 + 0.9*9.993 - 9.993)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.993 → 9.993\n",
      "    Formula: 9.993 + 0.1*(1 + 0.9*9.993 - 9.993)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.993 → 9.993\n",
      "    Formula: 9.993 + 0.1*(1 + 0.9*9.993 - 9.993)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.993 → 9.994\n",
      "    Formula: 9.993 + 0.1*(1 + 0.9*9.993 - 9.993)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.232 → 9.308\n",
      "    Formula: 9.232 + 0.1*(1 + 0.9*9.994 - 9.232)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.994 → 9.994\n",
      "    Formula: 9.994 + 0.1*(1 + 0.9*9.994 - 9.994)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 210: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 210 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 220 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.720 → 9.722\n",
      "    Formula: 9.720 + 0.1*(1 + 0.9*9.720 - 9.720)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.722 → 9.750\n",
      "    Formula: 9.722 + 0.1*(1 + 0.9*10.000 - 9.722)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 8.427 → 8.584\n",
      "    Formula: 8.427 + 0.1*(1 + 0.9*10.000 - 8.427)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "Episode 220: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 220 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 230 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.607 → 7.846\n",
      "    Formula: 7.607 + 0.1*(1 + 0.9*9.995 - 7.607)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.995\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.995 → 9.996\n",
      "    Formula: 9.995 + 0.1*(1 + 0.9*9.995 - 9.995)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.996 → 9.996\n",
      "    Formula: 9.996 + 0.1*(1 + 0.9*9.996 - 9.996)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.846 → 7.239\n",
      "    Formula: 7.846 + 0.1*(1 + 0.9*0.865 - 7.846)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 0.865 → 0.956\n",
      "    Formula: 0.865 + 0.1*(1 + 0.9*0.865 - 0.865)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "Episode 230: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 230 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 240 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -44.170 → -38.753\n",
      "    Formula: -44.170 + 0.1*(1 + 0.9*10.000 - -44.170)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 240: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 240 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 250 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 7.386 → 7.647\n",
      "    Formula: 7.386 + 0.1*(1 + 0.9*9.998 - 7.386)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "Episode 250: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 250 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 260 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 260: Reward = 20, Non-zero Q-values = 32\n",
      "\n",
      "Episode 260 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 270 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.164 → 9.247\n",
      "    Formula: 9.164 + 0.1*(1 + 0.9*10.000 - 9.164)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 25.0 → 30.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.247 → 8.587\n",
      "    Formula: 9.247 + 0.1*(1 + 0.9*1.821 - 9.247)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 1.821 → 1.903\n",
      "    Formula: 1.821 + 0.1*(1 + 0.9*1.821 - 1.821)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 1.903 → 1.984\n",
      "    Formula: 1.903 + 0.1*(1 + 0.9*1.903 - 1.903)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 1.984 → 2.064\n",
      "    Formula: 1.984 + 0.1*(1 + 0.9*1.984 - 1.984)\n",
      "    Step summary: State [30.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 30.0 → 35.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][1]: 0.000 → 0.286\n",
      "    Formula: 0.000 + 0.1*(1 + 0.9*2.064 - 0.000)\n",
      "    Step summary: State [35.0, 5.0], Step reward: 1\n",
      "Episode 270: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 270 finished. Total reward: 20\n",
      "Final state: 35.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 280 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.191 → 9.272\n",
      "    Formula: 9.191 + 0.1*(1 + 0.9*10.000 - 9.191)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.289 → 8.460\n",
      "    Formula: 8.289 + 0.1*(1 + 0.9*10.000 - 8.289)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.460 → 8.012\n",
      "    Formula: 8.460 + 0.1*(1 + 0.9*3.310 - 8.460)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.310 → 3.377\n",
      "    Formula: 3.310 + 0.1*(1 + 0.9*3.310 - 3.310)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.377 → 3.443\n",
      "    Formula: 3.377 + 0.1*(1 + 0.9*3.377 - 3.377)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.443 → 3.509\n",
      "    Formula: 3.443 + 0.1*(1 + 0.9*3.443 - 3.443)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.509 → 3.574\n",
      "    Formula: 3.509 + 0.1*(1 + 0.9*3.509 - 3.509)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.574 → 3.638\n",
      "    Formula: 3.574 + 0.1*(1 + 0.9*3.574 - 3.574)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.638 → 3.702\n",
      "    Formula: 3.638 + 0.1*(1 + 0.9*3.638 - 3.638)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.702 → 3.765\n",
      "    Formula: 3.702 + 0.1*(1 + 0.9*3.702 - 3.702)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "Episode 280: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 280 finished. Total reward: 20\n",
      "Final state: 90.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 290 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "Episode 290: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 290 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 300 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 7.505 → 7.755\n",
      "    Formula: 7.505 + 0.1*(1 + 0.9*10.000 - 7.505)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "Episode 300: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 300 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 310 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 310: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 310 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 320 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.710 → 7.939\n",
      "    Formula: 7.710 + 0.1*(1 + 0.9*9.999 - 7.710)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.939 → 7.394\n",
      "    Formula: 7.939 + 0.1*(1 + 0.9*1.655 - 7.939)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.655 → 1.738\n",
      "    Formula: 1.655 + 0.1*(1 + 0.9*1.655 - 1.655)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.738 → 1.821\n",
      "    Formula: 1.738 + 0.1*(1 + 0.9*1.738 - 1.738)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.821 → 1.903\n",
      "    Formula: 1.821 + 0.1*(1 + 0.9*1.821 - 1.821)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.903 → 1.984\n",
      "    Formula: 1.903 + 0.1*(1 + 0.9*1.903 - 1.903)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.984 → 2.064\n",
      "    Formula: 1.984 + 0.1*(1 + 0.9*1.984 - 1.984)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.064 → 2.143\n",
      "    Formula: 2.064 + 0.1*(1 + 0.9*2.064 - 2.064)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.143 → 2.222\n",
      "    Formula: 2.143 + 0.1*(1 + 0.9*2.143 - 2.143)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.222 → 2.300\n",
      "    Formula: 2.222 + 0.1*(1 + 0.9*2.222 - 2.222)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.300 → 2.377\n",
      "    Formula: 2.300 + 0.1*(1 + 0.9*2.300 - 2.300)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.377 → 2.453\n",
      "    Formula: 2.377 + 0.1*(1 + 0.9*2.377 - 2.377)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "Episode 320: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 320 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 330 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 330: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 330 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 340 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -61.571 → -54.414\n",
      "    Formula: -61.571 + 0.1*(1 + 0.9*10.000 - -61.571)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -54.414 → -58.073\n",
      "    Formula: -54.414 + 0.1*(-100 + 0.9*10.000 - -54.414)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 340: Reward = -89, Non-zero Q-values = 33\n",
      "\n",
      "Episode 340 finished. Total reward: -89\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 350 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.654 → 7.889\n",
      "    Formula: 7.654 + 0.1*(1 + 0.9*10.000 - 7.654)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 350: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 350 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 360 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 8.616 → 8.754\n",
      "    Formula: 8.616 + 0.1*(1 + 0.9*10.000 - 8.616)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.100 → 8.290\n",
      "    Formula: 8.100 + 0.1*(1 + 0.9*10.000 - 8.100)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.290 → 7.782\n",
      "    Formula: 8.290 + 0.1*(1 + 0.9*2.453 - 8.290)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.453 → 2.528\n",
      "    Formula: 2.453 + 0.1*(1 + 0.9*2.453 - 2.453)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.528 → 2.603\n",
      "    Formula: 2.528 + 0.1*(1 + 0.9*2.528 - 2.528)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.603 → 2.677\n",
      "    Formula: 2.603 + 0.1*(1 + 0.9*2.603 - 2.603)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.677 → 2.750\n",
      "    Formula: 2.677 + 0.1*(1 + 0.9*2.677 - 2.677)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.750 → 2.823\n",
      "    Formula: 2.750 + 0.1*(1 + 0.9*2.750 - 2.750)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "Episode 360: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 360 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 370 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.687 → 9.718\n",
      "    Formula: 9.687 + 0.1*(1 + 0.9*10.000 - 9.687)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "Episode 370: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 370 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 380 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.916 → 9.924\n",
      "    Formula: 9.916 + 0.1*(1 + 0.9*10.000 - 9.916)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.164 → 8.347\n",
      "    Formula: 8.164 + 0.1*(1 + 0.9*10.000 - 8.164)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.924 → 9.932\n",
      "    Formula: 9.924 + 0.1*(1 + 0.9*10.000 - 9.924)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 11.0 → 9.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.932 → 9.939\n",
      "    Formula: 9.932 + 0.1*(1 + 0.9*10.000 - 9.932)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "Episode 380: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 380 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 390 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.347 → 8.512\n",
      "    Formula: 8.347 + 0.1*(1 + 0.9*10.000 - 8.347)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.939 → 9.945\n",
      "    Formula: 9.939 + 0.1*(1 + 0.9*10.000 - 9.939)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.512 → 8.121\n",
      "    Formula: 8.512 + 0.1*(1 + 0.9*3.998 - 8.512)\n",
      "    Step summary: State [70.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 3.998 → 4.058\n",
      "    Formula: 3.998 + 0.1*(1 + 0.9*3.998 - 3.998)\n",
      "    Step summary: State [75.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.058 → 4.273\n",
      "    Formula: 4.058 + 0.1*(1 + 0.9*5.787 - 4.058)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "Episode 390: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 390 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 400 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.997 → 9.998\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*10.000 - 9.997)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 400: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 400 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 410 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*10.000 - 9.998)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 410: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 410 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 420 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*10.000 - 9.998)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 8.852 → 8.967\n",
      "    Formula: 8.852 + 0.1*(1 + 0.9*10.000 - 8.852)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "Episode 420: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 420 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 430 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.222 → 9.300\n",
      "    Formula: 9.222 + 0.1*(1 + 0.9*10.000 - 9.222)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.865 → 9.879\n",
      "    Formula: 9.865 + 0.1*(1 + 0.9*10.000 - 9.865)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 430: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 430 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 440 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 440: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 440 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 450 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 450: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 450 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 460 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [15.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][0]: 3.221 → 3.899\n",
      "    Formula: 3.221 + 0.1*(1 + 0.9*9.999 - 3.221)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 460: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 460 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.289 → 8.460\n",
      "    Formula: 8.289 + 0.1*(1 + 0.9*10.000 - 8.289)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.460 → 8.012\n",
      "    Formula: 8.460 + 0.1*(1 + 0.9*3.310 - 8.460)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.310 → 3.377\n",
      "    Formula: 3.310 + 0.1*(1 + 0.9*3.310 - 3.310)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.377 → 3.443\n",
      "    Formula: 3.377 + 0.1*(1 + 0.9*3.377 - 3.377)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.443 → 3.509\n",
      "    Formula: 3.443 + 0.1*(1 + 0.9*3.443 - 3.443)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.509 → 3.574\n",
      "    Formula: 3.509 + 0.1*(1 + 0.9*3.509 - 3.509)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.574 → 3.638\n",
      "    Formula: 3.574 + 0.1*(1 + 0.9*3.574 - 3.574)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.638 → 3.702\n",
      "    Formula: 3.638 + 0.1*(1 + 0.9*3.638 - 3.638)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 3.702 → 3.765\n",
      "    Formula: 3.702 + 0.1*(1 + 0.9*3.702 - 3.702)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "Episode 280: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 280 finished. Total reward: 20\n",
      "Final state: 90.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 290 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "Episode 290: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 290 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 300 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 7.505 → 7.755\n",
      "    Formula: 7.505 + 0.1*(1 + 0.9*10.000 - 7.505)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "Episode 300: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 300 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 310 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 310: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 310 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 320 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.710 → 7.939\n",
      "    Formula: 7.710 + 0.1*(1 + 0.9*9.999 - 7.710)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.939 → 7.394\n",
      "    Formula: 7.939 + 0.1*(1 + 0.9*1.655 - 7.939)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.655 → 1.738\n",
      "    Formula: 1.655 + 0.1*(1 + 0.9*1.655 - 1.655)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.738 → 1.821\n",
      "    Formula: 1.738 + 0.1*(1 + 0.9*1.738 - 1.738)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.821 → 1.903\n",
      "    Formula: 1.821 + 0.1*(1 + 0.9*1.821 - 1.821)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.903 → 1.984\n",
      "    Formula: 1.903 + 0.1*(1 + 0.9*1.903 - 1.903)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 1.984 → 2.064\n",
      "    Formula: 1.984 + 0.1*(1 + 0.9*1.984 - 1.984)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.064 → 2.143\n",
      "    Formula: 2.064 + 0.1*(1 + 0.9*2.064 - 2.064)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.143 → 2.222\n",
      "    Formula: 2.143 + 0.1*(1 + 0.9*2.143 - 2.143)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.222 → 2.300\n",
      "    Formula: 2.222 + 0.1*(1 + 0.9*2.222 - 2.222)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.300 → 2.377\n",
      "    Formula: 2.300 + 0.1*(1 + 0.9*2.300 - 2.300)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.377 → 2.453\n",
      "    Formula: 2.377 + 0.1*(1 + 0.9*2.377 - 2.377)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "Episode 320: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 320 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 330 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 330: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 330 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 340 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -61.571 → -54.414\n",
      "    Formula: -61.571 + 0.1*(1 + 0.9*10.000 - -61.571)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -54.414 → -58.073\n",
      "    Formula: -54.414 + 0.1*(-100 + 0.9*10.000 - -54.414)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 340: Reward = -89, Non-zero Q-values = 33\n",
      "\n",
      "Episode 340 finished. Total reward: -89\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 350 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 7.654 → 7.889\n",
      "    Formula: 7.654 + 0.1*(1 + 0.9*10.000 - 7.654)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 350: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 350 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 360 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 8.616 → 8.754\n",
      "    Formula: 8.616 + 0.1*(1 + 0.9*10.000 - 8.616)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.100 → 8.290\n",
      "    Formula: 8.100 + 0.1*(1 + 0.9*10.000 - 8.100)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.290 → 7.782\n",
      "    Formula: 8.290 + 0.1*(1 + 0.9*2.453 - 8.290)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.453 → 2.528\n",
      "    Formula: 2.453 + 0.1*(1 + 0.9*2.453 - 2.453)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.528 → 2.603\n",
      "    Formula: 2.528 + 0.1*(1 + 0.9*2.528 - 2.528)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.603 → 2.677\n",
      "    Formula: 2.603 + 0.1*(1 + 0.9*2.603 - 2.603)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.677 → 2.750\n",
      "    Formula: 2.677 + 0.1*(1 + 0.9*2.677 - 2.677)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 2.750 → 2.823\n",
      "    Formula: 2.750 + 0.1*(1 + 0.9*2.750 - 2.750)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "Episode 360: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 360 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 370 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.687 → 9.718\n",
      "    Formula: 9.687 + 0.1*(1 + 0.9*10.000 - 9.687)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "Episode 370: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 370 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 380 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.916 → 9.924\n",
      "    Formula: 9.916 + 0.1*(1 + 0.9*10.000 - 9.916)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.164 → 8.347\n",
      "    Formula: 8.164 + 0.1*(1 + 0.9*10.000 - 8.164)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.924 → 9.932\n",
      "    Formula: 9.924 + 0.1*(1 + 0.9*10.000 - 9.924)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 11.0 → 9.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.932 → 9.939\n",
      "    Formula: 9.932 + 0.1*(1 + 0.9*10.000 - 9.932)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 9.0], Step reward: 1\n",
      "Episode 380: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 380 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 390 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.347 → 8.512\n",
      "    Formula: 8.347 + 0.1*(1 + 0.9*10.000 - 8.347)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.939 → 9.945\n",
      "    Formula: 9.939 + 0.1*(1 + 0.9*10.000 - 9.939)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.512 → 8.121\n",
      "    Formula: 8.512 + 0.1*(1 + 0.9*3.998 - 8.512)\n",
      "    Step summary: State [70.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 3.998 → 4.058\n",
      "    Formula: 3.998 + 0.1*(1 + 0.9*3.998 - 3.998)\n",
      "    Step summary: State [75.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.058 → 4.273\n",
      "    Formula: 4.058 + 0.1*(1 + 0.9*5.787 - 4.058)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "Episode 390: Reward = 20, Non-zero Q-values = 33\n",
      "\n",
      "Episode 390 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 400 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.997 → 9.997\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*9.997 - 9.997)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.997 → 9.998\n",
      "    Formula: 9.997 + 0.1*(1 + 0.9*10.000 - 9.997)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 400: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 400 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 410 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*10.000 - 9.998)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 410: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 410 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 420 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*9.998 - 9.998)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.998 → 9.998\n",
      "    Formula: 9.998 + 0.1*(1 + 0.9*10.000 - 9.998)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 8.852 → 8.967\n",
      "    Formula: 8.852 + 0.1*(1 + 0.9*10.000 - 8.852)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "Episode 420: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 420 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 430 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.222 → 9.300\n",
      "    Formula: 9.222 + 0.1*(1 + 0.9*10.000 - 9.222)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.865 → 9.879\n",
      "    Formula: 9.865 + 0.1*(1 + 0.9*10.000 - 9.865)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 430: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 430 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 440 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 440: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 440 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 450 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 450: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 450 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 460 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*9.999 - 9.999)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [15.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][0]: 3.221 → 3.899\n",
      "    Formula: 3.221 + 0.1*(1 + 0.9*9.999 - 3.221)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 460: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 460 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 470 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 470: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 470 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 480 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -21.273 → -28.246\n",
      "    Formula: -21.273 + 0.1*(-100 + 0.9*10.000 - -21.273)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 480: Reward = -94, Non-zero Q-values = 35\n",
      "\n",
      "Episode 480 finished. Total reward: -94\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 490 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -27.981 → -24.183\n",
      "    Formula: -27.981 + 0.1*(1 + 0.9*10.000 - -27.981)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -24.183 → -30.865\n",
      "    Formula: -24.183 + 0.1*(-100 + 0.9*10.000 - -24.183)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 490: Reward = -92, Non-zero Q-values = 35\n",
      "\n",
      "Episode 490 finished. Total reward: -92\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 500 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 500: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 500 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 510 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.563 → 8.706\n",
      "    Formula: 8.563 + 0.1*(1 + 0.9*10.000 - 8.563)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.706 → 8.363\n",
      "    Formula: 8.706 + 0.1*(1 + 0.9*4.748 - 8.706)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.748 → 4.800\n",
      "    Formula: 4.748 + 0.1*(1 + 0.9*4.748 - 4.748)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.800 → 4.981\n",
      "    Formula: 4.800 + 0.1*(1 + 0.9*6.228 - 4.800)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 510: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 510 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 520 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.279 → 8.451\n",
      "    Formula: 8.279 + 0.1*(1 + 0.9*10.000 - 8.279)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.269 → 9.342\n",
      "    Formula: 9.269 + 0.1*(1 + 0.9*10.000 - 9.269)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "Episode 520: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 520 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 530 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.451 → 8.606\n",
      "    Formula: 8.451 + 0.1*(1 + 0.9*10.000 - 8.451)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 530: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 530 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 540 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.827 → 9.844\n",
      "    Formula: 9.827 + 0.1*(1 + 0.9*10.000 - 9.827)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.527 → 8.674\n",
      "    Formula: 8.527 + 0.1*(1 + 0.9*10.000 - 8.527)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "Episode 540: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 540 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 550 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.860 → 9.874\n",
      "    Formula: 9.860 + 0.1*(1 + 0.9*10.000 - 9.860)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 550: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 550 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 560 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 560: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 560 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 570 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.962 → 9.966\n",
      "    Formula: 9.962 + 0.1*(1 + 0.9*10.000 - 9.962)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.966 → 9.969\n",
      "    Formula: 9.966 + 0.1*(1 + 0.9*10.000 - 9.966)\n",
      "    Step summary: State [80.0, 16.0], Step reward: 1\n",
      "Episode 570: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 570 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 16.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 580 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.898 → 9.908\n",
      "    Formula: 9.898 + 0.1*(1 + 0.9*10.000 - 9.898)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.908 → 9.917\n",
      "    Formula: 9.908 + 0.1*(1 + 0.9*10.000 - 9.908)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "Episode 580: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 580 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 590 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.576 → 8.719\n",
      "    Formula: 8.576 + 0.1*(1 + 0.9*10.000 - 8.576)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "Episode 590: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 590 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 600 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -39.973 → -34.976\n",
      "    Formula: -39.973 + 0.1*(1 + 0.9*10.000 - -39.973)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -34.976 → -40.578\n",
      "    Formula: -34.976 + 0.1*(-100 + 0.9*10.000 - -34.976)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 600: Reward = -94, Non-zero Q-values = 35\n",
      "\n",
      "Episode 600 finished. Total reward: -94\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 610 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -49.737 → -43.763\n",
      "    Formula: -49.737 + 0.1*(1 + 0.9*10.000 - -49.737)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 610: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 610 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 620 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.246 → 9.322\n",
      "    Formula: 9.246 + 0.1*(1 + 0.9*10.000 - 9.246)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "Episode 620: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 620 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 630 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "--- EPISODE 470 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 470: Reward = 20, Non-zero Q-values = 34\n",
      "\n",
      "Episode 470 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 480 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -21.273 → -28.246\n",
      "    Formula: -21.273 + 0.1*(-100 + 0.9*10.000 - -21.273)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 480: Reward = -94, Non-zero Q-values = 35\n",
      "\n",
      "Episode 480 finished. Total reward: -94\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 490 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -27.981 → -24.183\n",
      "    Formula: -27.981 + 0.1*(1 + 0.9*10.000 - -27.981)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -24.183 → -30.865\n",
      "    Formula: -24.183 + 0.1*(-100 + 0.9*10.000 - -24.183)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 490: Reward = -92, Non-zero Q-values = 35\n",
      "\n",
      "Episode 490 finished. Total reward: -92\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 500 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 500: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 500 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 510 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.563 → 8.706\n",
      "    Formula: 8.563 + 0.1*(1 + 0.9*10.000 - 8.563)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.706 → 8.363\n",
      "    Formula: 8.706 + 0.1*(1 + 0.9*4.748 - 8.706)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.748 → 4.800\n",
      "    Formula: 4.748 + 0.1*(1 + 0.9*4.748 - 4.748)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 4.800 → 4.981\n",
      "    Formula: 4.800 + 0.1*(1 + 0.9*6.228 - 4.800)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 510: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 510 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 520 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.279 → 8.451\n",
      "    Formula: 8.279 + 0.1*(1 + 0.9*10.000 - 8.279)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.269 → 9.342\n",
      "    Formula: 9.269 + 0.1*(1 + 0.9*10.000 - 9.269)\n",
      "    Step summary: State [45.0, 7.0], Step reward: 1\n",
      "Episode 520: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 520 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 530 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.451 → 8.606\n",
      "    Formula: 8.451 + 0.1*(1 + 0.9*10.000 - 8.451)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 530: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 530 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 540 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.827 → 9.844\n",
      "    Formula: 9.827 + 0.1*(1 + 0.9*10.000 - 9.827)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.527 → 8.674\n",
      "    Formula: 8.527 + 0.1*(1 + 0.9*10.000 - 8.527)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "Episode 540: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 540 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 550 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.860 → 9.874\n",
      "    Formula: 9.860 + 0.1*(1 + 0.9*10.000 - 9.860)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 550: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 550 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 560 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 560: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 560 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 570 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.962 → 9.966\n",
      "    Formula: 9.962 + 0.1*(1 + 0.9*10.000 - 9.962)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.966 → 9.969\n",
      "    Formula: 9.966 + 0.1*(1 + 0.9*10.000 - 9.966)\n",
      "    Step summary: State [80.0, 16.0], Step reward: 1\n",
      "Episode 570: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 570 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 16.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 580 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.898 → 9.908\n",
      "    Formula: 9.898 + 0.1*(1 + 0.9*10.000 - 9.898)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.908 → 9.917\n",
      "    Formula: 9.908 + 0.1*(1 + 0.9*10.000 - 9.908)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 11.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 11.0], Step reward: 1\n",
      "Episode 580: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 580 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 590 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.576 → 8.719\n",
      "    Formula: 8.576 + 0.1*(1 + 0.9*10.000 - 8.576)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "Episode 590: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 590 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 600 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -39.973 → -34.976\n",
      "    Formula: -39.973 + 0.1*(1 + 0.9*10.000 - -39.973)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -34.976 → -40.578\n",
      "    Formula: -34.976 + 0.1*(-100 + 0.9*10.000 - -34.976)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 600: Reward = -94, Non-zero Q-values = 35\n",
      "\n",
      "Episode 600 finished. Total reward: -94\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 610 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -49.737 → -43.763\n",
      "    Formula: -49.737 + 0.1*(1 + 0.9*10.000 - -49.737)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 610: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 610 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 620 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.246 → 9.322\n",
      "    Formula: 9.246 + 0.1*(1 + 0.9*10.000 - 9.246)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 5.0], Step reward: 1\n",
      "Episode 620: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 620 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 630 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.900 → 9.010\n",
      "    Formula: 8.900 + 0.1*(1 + 0.9*10.000 - 8.900)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 630: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 630 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 640 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.985 → 9.987\n",
      "    Formula: 9.985 + 0.1*(1 + 0.9*10.000 - 9.985)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.308 → 8.477\n",
      "    Formula: 8.308 + 0.1*(1 + 0.9*10.000 - 8.308)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 640: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 640 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 650 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.278 → 9.351\n",
      "    Formula: 9.278 + 0.1*(1 + 0.9*10.000 - 9.278)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.351 → 9.044\n",
      "    Formula: 9.351 + 0.1*(1 + 0.9*5.871 - 9.351)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.871 → 5.912\n",
      "    Formula: 5.871 + 0.1*(1 + 0.9*5.871 - 5.871)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][1]: 1.063 → 1.589\n",
      "    Formula: 1.063 + 0.1*(1 + 0.9*5.912 - 1.063)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.912 → 5.953\n",
      "    Formula: 5.912 + 0.1*(1 + 0.9*5.912 - 5.912)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.953 → 5.993\n",
      "    Formula: 5.953 + 0.1*(1 + 0.9*5.953 - 5.953)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.993 → 6.033\n",
      "    Formula: 5.993 + 0.1*(1 + 0.9*5.993 - 5.993)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.033 → 6.073\n",
      "    Formula: 6.033 + 0.1*(1 + 0.9*6.033 - 6.033)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.073 → 6.112\n",
      "    Formula: 6.073 + 0.1*(1 + 0.9*6.073 - 6.073)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.112 → 6.151\n",
      "    Formula: 6.112 + 0.1*(1 + 0.9*6.112 - 6.112)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.151 → 6.190\n",
      "    Formula: 6.151 + 0.1*(1 + 0.9*6.151 - 6.151)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.190 → 6.228\n",
      "    Formula: 6.190 + 0.1*(1 + 0.9*6.190 - 6.190)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.228 → 6.265\n",
      "    Formula: 6.228 + 0.1*(1 + 0.9*6.228 - 6.228)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "Episode 650: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 650 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 660 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.343 → 8.508\n",
      "    Formula: 8.343 + 0.1*(1 + 0.9*10.000 - 8.343)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 8.900 → 9.010\n",
      "    Formula: 8.900 + 0.1*(1 + 0.9*10.000 - 8.900)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 630: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 630 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 640 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.985 → 9.987\n",
      "    Formula: 9.985 + 0.1*(1 + 0.9*10.000 - 9.985)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.308 → 8.477\n",
      "    Formula: 8.308 + 0.1*(1 + 0.9*10.000 - 8.308)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 640: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 640 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 650 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.278 → 9.351\n",
      "    Formula: 9.278 + 0.1*(1 + 0.9*10.000 - 9.278)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.351 → 9.044\n",
      "    Formula: 9.351 + 0.1*(1 + 0.9*5.871 - 9.351)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.871 → 5.912\n",
      "    Formula: 5.871 + 0.1*(1 + 0.9*5.871 - 5.871)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][1]: 1.063 → 1.589\n",
      "    Formula: 1.063 + 0.1*(1 + 0.9*5.912 - 1.063)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.912 → 5.953\n",
      "    Formula: 5.912 + 0.1*(1 + 0.9*5.912 - 5.912)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.953 → 5.993\n",
      "    Formula: 5.953 + 0.1*(1 + 0.9*5.953 - 5.953)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 5.993 → 6.033\n",
      "    Formula: 5.993 + 0.1*(1 + 0.9*5.993 - 5.993)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.033 → 6.073\n",
      "    Formula: 6.033 + 0.1*(1 + 0.9*6.033 - 6.033)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.073 → 6.112\n",
      "    Formula: 6.073 + 0.1*(1 + 0.9*6.073 - 6.073)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.112 → 6.151\n",
      "    Formula: 6.112 + 0.1*(1 + 0.9*6.112 - 6.112)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.151 → 6.190\n",
      "    Formula: 6.151 + 0.1*(1 + 0.9*6.151 - 6.151)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.190 → 6.228\n",
      "    Formula: 6.190 + 0.1*(1 + 0.9*6.190 - 6.190)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 6.228 → 6.265\n",
      "    Formula: 6.228 + 0.1*(1 + 0.9*6.228 - 6.228)\n",
      "    Step summary: State [45.0, 5.0], Step reward: 1\n",
      "Episode 650: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 650 finished. Total reward: 20\n",
      "Final state: 45.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 660 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.343 → 8.508\n",
      "    Formula: 8.343 + 0.1*(1 + 0.9*10.000 - 8.343)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "Episode 660: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 660 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 670 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.990 → 9.991\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*10.000 - 9.990)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.658 → 8.792\n",
      "    Formula: 8.658 + 0.1*(1 + 0.9*10.000 - 8.658)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.991 → 9.992\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*10.000 - 9.991)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 16.0 → 14.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.992 → 9.673\n",
      "    Formula: 9.992 + 0.1*(1 + 0.9*6.448 - 9.992)\n",
      "    Step summary: State [85.0, 14.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 14.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.448 → 6.484\n",
      "    Formula: 6.448 + 0.1*(1 + 0.9*6.448 - 6.448)\n",
      "    Step summary: State [85.0, 14.0], Step reward: 1\n",
      "Episode 670: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 670 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 14.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 680 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "Episode 660: Reward = 20, Non-zero Q-values = 35\n",
      "\n",
      "Episode 660 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 670 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.990 → 9.991\n",
      "    Formula: 9.990 + 0.1*(1 + 0.9*10.000 - 9.990)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.658 → 8.792\n",
      "    Formula: 8.658 + 0.1*(1 + 0.9*10.000 - 8.658)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 18.0 → 16.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.991 → 9.992\n",
      "    Formula: 9.991 + 0.1*(1 + 0.9*10.000 - 9.991)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 16.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 16.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 16.0 → 14.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.992 → 9.673\n",
      "    Formula: 9.992 + 0.1*(1 + 0.9*6.448 - 9.992)\n",
      "    Step summary: State [85.0, 14.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 14.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.448 → 6.484\n",
      "    Formula: 6.448 + 0.1*(1 + 0.9*6.448 - 6.448)\n",
      "    Step summary: State [85.0, 14.0], Step reward: 1\n",
      "Episode 670: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 670 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 14.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 680 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 680: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 680 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 690 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -52.946 → -56.752\n",
      "    Formula: -52.946 + 0.1*(-100 + 0.9*10.000 - -52.946)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 690: Reward = -88, Non-zero Q-values = 36\n",
      "\n",
      "Episode 690 finished. Total reward: -88\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 700 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -51.067 → -55.060\n",
      "    Formula: -51.067 + 0.1*(-100 + 0.9*10.000 - -51.067)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 700: Reward = -90, Non-zero Q-values = 36\n",
      "\n",
      "Episode 700 finished. Total reward: -90\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 710 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.192 → 9.273\n",
      "    Formula: 9.192 + 0.1*(1 + 0.9*10.000 - 9.192)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.273 → 8.963\n",
      "    Formula: 9.273 + 0.1*(1 + 0.9*5.745 - 9.273)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][2]: 1.045 → 1.558\n",
      "    Formula: 1.045 + 0.1*(1 + 0.9*5.745 - 1.045)\n",
      "    Step summary: State [70.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 5.745 → 5.788\n",
      "    Formula: 5.745 + 0.1*(1 + 0.9*5.745 - 5.745)\n",
      "    Step summary: State [75.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 5.788 → 5.911\n",
      "    Formula: 5.788 + 0.1*(1 + 0.9*6.690 - 5.788)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.690 → 6.723\n",
      "    Formula: 6.690 + 0.1*(1 + 0.9*6.690 - 6.690)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.723 → 6.756\n",
      "    Formula: 6.723 + 0.1*(1 + 0.9*6.723 - 6.723)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.756 → 6.788\n",
      "    Formula: 6.756 + 0.1*(1 + 0.9*6.756 - 6.756)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.788 → 6.820\n",
      "    Formula: 6.788 + 0.1*(1 + 0.9*6.788 - 6.788)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.820 → 6.852\n",
      "    Formula: 6.820 + 0.1*(1 + 0.9*6.820 - 6.820)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 680: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 680 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 690 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -52.946 → -56.752\n",
      "    Formula: -52.946 + 0.1*(-100 + 0.9*10.000 - -52.946)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 690: Reward = -88, Non-zero Q-values = 36\n",
      "\n",
      "Episode 690 finished. Total reward: -88\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 700 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -51.067 → -55.060\n",
      "    Formula: -51.067 + 0.1*(-100 + 0.9*10.000 - -51.067)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 700: Reward = -90, Non-zero Q-values = 36\n",
      "\n",
      "Episode 700 finished. Total reward: -90\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 710 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.192 → 9.273\n",
      "    Formula: 9.192 + 0.1*(1 + 0.9*10.000 - 9.192)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.273 → 8.963\n",
      "    Formula: 9.273 + 0.1*(1 + 0.9*5.745 - 9.273)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][2]: 1.045 → 1.558\n",
      "    Formula: 1.045 + 0.1*(1 + 0.9*5.745 - 1.045)\n",
      "    Step summary: State [70.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 5.745 → 5.788\n",
      "    Formula: 5.745 + 0.1*(1 + 0.9*5.745 - 5.745)\n",
      "    Step summary: State [75.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 5.788 → 5.911\n",
      "    Formula: 5.788 + 0.1*(1 + 0.9*6.690 - 5.788)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.690 → 6.723\n",
      "    Formula: 6.690 + 0.1*(1 + 0.9*6.690 - 6.690)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.723 → 6.756\n",
      "    Formula: 6.723 + 0.1*(1 + 0.9*6.723 - 6.723)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.756 → 6.788\n",
      "    Formula: 6.756 + 0.1*(1 + 0.9*6.756 - 6.756)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.788 → 6.820\n",
      "    Formula: 6.788 + 0.1*(1 + 0.9*6.788 - 6.788)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.820 → 6.852\n",
      "    Formula: 6.820 + 0.1*(1 + 0.9*6.820 - 6.820)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.852 → 6.883\n",
      "    Formula: 6.852 + 0.1*(1 + 0.9*6.852 - 6.852)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.883 → 6.915\n",
      "    Formula: 6.883 + 0.1*(1 + 0.9*6.883 - 6.883)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 6.915 → 6.945\n",
      "    Formula: 6.915 + 0.1*(1 + 0.9*6.915 - 6.915)\n",
      "    Step summary: State [80.0, 13.0], Step reward: 1\n",
      "Episode 710: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 710 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 720 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.735 → 9.762\n",
      "    Formula: 9.735 + 0.1*(1 + 0.9*10.000 - 9.735)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 18.0], Step reward: 1\n",
      "Episode 720: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 720 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 730 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 730: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 730 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 740 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -41.368 → -46.331\n",
      "    Formula: -41.368 + 0.1*(-100 + 0.9*10.000 - -41.368)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 740: Reward = -83, Non-zero Q-values = 36\n",
      "\n",
      "Episode 740 finished. Total reward: -83\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 750 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -46.331 → -50.798\n",
      "    Formula: -46.331 + 0.1*(-100 + 0.9*10.000 - -46.331)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 750: Reward = -96, Non-zero Q-values = 36\n",
      "\n",
      "Episode 750 finished. Total reward: -96\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 760 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.904 → 9.014\n",
      "    Formula: 8.904 + 0.1*(1 + 0.9*10.000 - 8.904)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.014 → 8.776\n",
      "    Formula: 9.014 + 0.1*(1 + 0.9*6.267 - 9.014)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 6.267 → 6.304\n",
      "    Formula: 6.267 + 0.1*(1 + 0.9*6.267 - 6.267)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 6.304 → 6.428\n",
      "    Formula: 6.304 + 0.1*(1 + 0.9*7.265 - 6.304)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.265 → 7.292\n",
      "    Formula: 7.265 + 0.1*(1 + 0.9*7.265 - 7.265)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.292 → 7.320\n",
      "    Formula: 7.292 + 0.1*(1 + 0.9*7.292 - 7.292)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.320 → 7.346\n",
      "    Formula: 7.320 + 0.1*(1 + 0.9*7.320 - 7.320)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.346 → 7.373\n",
      "    Formula: 7.346 + 0.1*(1 + 0.9*7.346 - 7.346)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.373 → 7.399\n",
      "    Formula: 7.373 + 0.1*(1 + 0.9*7.373 - 7.373)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 760: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 760 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 770 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 730: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 730 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 740 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -41.368 → -46.331\n",
      "    Formula: -41.368 + 0.1*(-100 + 0.9*10.000 - -41.368)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 740: Reward = -83, Non-zero Q-values = 36\n",
      "\n",
      "Episode 740 finished. Total reward: -83\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 750 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -46.331 → -50.798\n",
      "    Formula: -46.331 + 0.1*(-100 + 0.9*10.000 - -46.331)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 750: Reward = -96, Non-zero Q-values = 36\n",
      "\n",
      "Episode 750 finished. Total reward: -96\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 760 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 8.904 → 9.014\n",
      "    Formula: 8.904 + 0.1*(1 + 0.9*10.000 - 8.904)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 65.0 → 70.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.014 → 8.776\n",
      "    Formula: 9.014 + 0.1*(1 + 0.9*6.267 - 9.014)\n",
      "    Step summary: State [70.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 70.0 → 75.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(7), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 6.267 → 6.304\n",
      "    Formula: 6.267 + 0.1*(1 + 0.9*6.267 - 6.267)\n",
      "    Step summary: State [75.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(7), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 75.0 → 80.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(7), np.int64(1))][1]: 6.304 → 6.428\n",
      "    Formula: 6.304 + 0.1*(1 + 0.9*7.265 - 6.304)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.265 → 7.292\n",
      "    Formula: 7.265 + 0.1*(1 + 0.9*7.265 - 7.265)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.292 → 7.320\n",
      "    Formula: 7.292 + 0.1*(1 + 0.9*7.292 - 7.292)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.320 → 7.346\n",
      "    Formula: 7.320 + 0.1*(1 + 0.9*7.320 - 7.320)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.346 → 7.373\n",
      "    Formula: 7.346 + 0.1*(1 + 0.9*7.346 - 7.346)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(1))\n",
      "    Q-update: Q[(np.int64(8), np.int64(1))][0]: 7.373 → 7.399\n",
      "    Formula: 7.373 + 0.1*(1 + 0.9*7.373 - 7.373)\n",
      "    Step summary: State [80.0, 15.0], Step reward: 1\n",
      "Episode 760: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 760 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 770 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 770: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 770 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 780 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.167 → 9.251\n",
      "    Formula: 9.167 + 0.1*(1 + 0.9*10.000 - 9.167)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 780: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 780 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 790 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "Episode 770: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 770 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 780 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.167 → 9.251\n",
      "    Formula: 9.167 + 0.1*(1 + 0.9*10.000 - 9.167)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 780: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 780 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 790 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 790: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 790 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 800 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -34.623 → -30.161\n",
      "    Formula: -34.623 + 0.1*(1 + 0.9*10.000 - -34.623)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -30.161 → -36.245\n",
      "    Formula: -30.161 + 0.1*(-100 + 0.9*10.000 - -30.161)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 800: Reward = -83, Non-zero Q-values = 36\n",
      "\n",
      "Episode 800 finished. Total reward: -83\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 810 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 810: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 810 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "Episode 790: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 790 finished. Total reward: 20\n",
      "Final state: 20.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 800 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -34.623 → -30.161\n",
      "    Formula: -34.623 + 0.1*(1 + 0.9*10.000 - -34.623)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -30.161 → -36.245\n",
      "    Formula: -30.161 + 0.1*(-100 + 0.9*10.000 - -30.161)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 800: Reward = -83, Non-zero Q-values = 36\n",
      "\n",
      "Episode 800 finished. Total reward: -83\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 810 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "Episode 810: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 810 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 820 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [15.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][0]: 7.374 → 7.636\n",
      "    Formula: 7.374 + 0.1*(1 + 0.9*10.000 - 7.374)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.824 → 9.842\n",
      "    Formula: 9.824 + 0.1*(1 + 0.9*10.000 - 9.824)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 25.0 → 30.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.842 → 9.761\n",
      "    Formula: 9.842 + 0.1*(1 + 0.9*8.926 - 9.842)\n",
      "    Step summary: State [30.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 8.926 → 8.937\n",
      "    Formula: 8.926 + 0.1*(1 + 0.9*8.926 - 8.926)\n",
      "    Step summary: State [30.0, 3.0], Step reward: 1\n",
      "Episode 820: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 820 finished. Total reward: 20\n",
      "Final state: 30.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 830 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.963 → 9.067\n",
      "    Formula: 8.963 + 0.1*(1 + 0.9*10.000 - 8.963)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.826 → 9.844\n",
      "    Formula: 9.826 + 0.1*(1 + 0.9*10.000 - 9.826)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.067 → 8.773\n",
      "    Formula: 9.067 + 0.1*(1 + 0.9*5.701 - 9.067)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.701 → 5.744\n",
      "    Formula: 5.701 + 0.1*(1 + 0.9*5.701 - 5.701)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.744 → 5.787\n",
      "    Formula: 5.744 + 0.1*(1 + 0.9*5.744 - 5.744)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.787 → 5.829\n",
      "    Formula: 5.787 + 0.1*(1 + 0.9*5.787 - 5.787)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.829 → 5.871\n",
      "    Formula: 5.829 + 0.1*(1 + 0.9*5.829 - 5.829)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.871 → 5.912\n",
      "    Formula: 5.871 + 0.1*(1 + 0.9*5.871 - 5.871)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.912 → 5.953\n",
      "    Formula: 5.912 + 0.1*(1 + 0.9*5.912 - 5.912)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.953 → 5.993\n",
      "    Formula: 5.953 + 0.1*(1 + 0.9*5.953 - 5.953)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.993 → 6.033\n",
      "    Formula: 5.993 + 0.1*(1 + 0.9*5.993 - 5.993)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.033 → 6.073\n",
      "    Formula: 6.033 + 0.1*(1 + 0.9*6.033 - 6.033)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "Episode 830: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 830 finished. Total reward: 20\n",
      "Final state: 90.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 820 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [15.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][0]: 7.374 → 7.636\n",
      "    Formula: 7.374 + 0.1*(1 + 0.9*10.000 - 7.374)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.824 → 9.842\n",
      "    Formula: 9.824 + 0.1*(1 + 0.9*10.000 - 9.824)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 25.0 → 30.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.842 → 9.761\n",
      "    Formula: 9.842 + 0.1*(1 + 0.9*8.926 - 9.842)\n",
      "    Step summary: State [30.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(3), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [30.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(3), np.int64(0))\n",
      "    Q-update: Q[(np.int64(3), np.int64(0))][0]: 8.926 → 8.937\n",
      "    Formula: 8.926 + 0.1*(1 + 0.9*8.926 - 8.926)\n",
      "    Step summary: State [30.0, 3.0], Step reward: 1\n",
      "Episode 820: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 820 finished. Total reward: 20\n",
      "Final state: 30.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 830 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.963 → 9.067\n",
      "    Formula: 8.963 + 0.1*(1 + 0.9*10.000 - 8.963)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.826 → 9.844\n",
      "    Formula: 9.826 + 0.1*(1 + 0.9*10.000 - 9.826)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.067 → 8.773\n",
      "    Formula: 9.067 + 0.1*(1 + 0.9*5.701 - 9.067)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.701 → 5.744\n",
      "    Formula: 5.701 + 0.1*(1 + 0.9*5.701 - 5.701)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.744 → 5.787\n",
      "    Formula: 5.744 + 0.1*(1 + 0.9*5.744 - 5.744)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.787 → 5.829\n",
      "    Formula: 5.787 + 0.1*(1 + 0.9*5.787 - 5.787)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.829 → 5.871\n",
      "    Formula: 5.829 + 0.1*(1 + 0.9*5.829 - 5.829)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.871 → 5.912\n",
      "    Formula: 5.871 + 0.1*(1 + 0.9*5.871 - 5.871)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.912 → 5.953\n",
      "    Formula: 5.912 + 0.1*(1 + 0.9*5.912 - 5.912)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.953 → 5.993\n",
      "    Formula: 5.953 + 0.1*(1 + 0.9*5.953 - 5.953)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 5.993 → 6.033\n",
      "    Formula: 5.993 + 0.1*(1 + 0.9*5.993 - 5.993)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 18.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.033 → 6.073\n",
      "    Formula: 6.033 + 0.1*(1 + 0.9*6.033 - 6.033)\n",
      "    Step summary: State [90.0, 18.0], Step reward: 1\n",
      "Episode 830: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 830 finished. Total reward: 20\n",
      "Final state: 90.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 840 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.773 → 8.896\n",
      "    Formula: 8.773 + 0.1*(1 + 0.9*10.000 - 8.773)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.896 → 8.653\n",
      "    Formula: 8.896 + 0.1*(1 + 0.9*6.073 - 8.896)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.073 → 6.112\n",
      "    Formula: 6.073 + 0.1*(1 + 0.9*6.073 - 6.073)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 90.0 → 95.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.460 → 1.964\n",
      "    Formula: 1.460 + 0.1*(1 + 0.9*6.112 - 1.460)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.112 → 6.151\n",
      "    Formula: 6.112 + 0.1*(1 + 0.9*6.112 - 6.112)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.151 → 6.190\n",
      "    Formula: 6.151 + 0.1*(1 + 0.9*6.151 - 6.151)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.190 → 6.228\n",
      "    Formula: 6.190 + 0.1*(1 + 0.9*6.190 - 6.190)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.228 → 6.265\n",
      "    Formula: 6.228 + 0.1*(1 + 0.9*6.228 - 6.228)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.265 → 6.303\n",
      "    Formula: 6.265 + 0.1*(1 + 0.9*6.265 - 6.265)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.303 → 6.340\n",
      "    Formula: 6.303 + 0.1*(1 + 0.9*6.303 - 6.303)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.340 → 6.376\n",
      "    Formula: 6.340 + 0.1*(1 + 0.9*6.340 - 6.340)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.376 → 6.413\n",
      "    Formula: 6.376 + 0.1*(1 + 0.9*6.376 - 6.376)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 95.0 → 100.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.964 → 1.894\n",
      "    Formula: 1.964 + 0.1*(1 + 0.9*0.297 - 1.964)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.297 → 0.394\n",
      "    Formula: 0.297 + 0.1*(1 + 0.9*0.297 - 0.297)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.585 → 0.679\n",
      "    Formula: 0.585 + 0.1*(1 + 0.9*0.585 - 0.585)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "Episode 840: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 840 finished. Total reward: 20\n",
      "Final state: 100.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 850 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.494 → 9.545\n",
      "    Formula: 9.494 + 0.1*(1 + 0.9*10.000 - 9.494)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 45.0 → 50.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.545 → 9.344\n",
      "    Formula: 9.545 + 0.1*(1 + 0.9*7.265 - 9.545)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.265 → 7.292\n",
      "    Formula: 7.265 + 0.1*(1 + 0.9*7.265 - 7.265)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.292 → 7.320\n",
      "    Formula: 7.292 + 0.1*(1 + 0.9*7.292 - 7.292)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.320 → 7.346\n",
      "    Formula: 7.320 + 0.1*(1 + 0.9*7.320 - 7.320)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.346 → 7.373\n",
      "    Formula: 7.346 + 0.1*(1 + 0.9*7.346 - 7.346)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.373 → 7.399\n",
      "    Formula: 7.373 + 0.1*(1 + 0.9*7.373 - 7.373)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "--- EPISODE 840 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.773 → 8.896\n",
      "    Formula: 8.773 + 0.1*(1 + 0.9*10.000 - 8.773)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 8.896 → 8.653\n",
      "    Formula: 8.896 + 0.1*(1 + 0.9*6.073 - 8.896)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.073 → 6.112\n",
      "    Formula: 6.073 + 0.1*(1 + 0.9*6.073 - 6.073)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 90.0 → 95.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.460 → 1.964\n",
      "    Formula: 1.460 + 0.1*(1 + 0.9*6.112 - 1.460)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.112 → 6.151\n",
      "    Formula: 6.112 + 0.1*(1 + 0.9*6.112 - 6.112)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.151 → 6.190\n",
      "    Formula: 6.151 + 0.1*(1 + 0.9*6.151 - 6.151)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.190 → 6.228\n",
      "    Formula: 6.190 + 0.1*(1 + 0.9*6.190 - 6.190)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.228 → 6.265\n",
      "    Formula: 6.228 + 0.1*(1 + 0.9*6.228 - 6.228)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.265 → 6.303\n",
      "    Formula: 6.265 + 0.1*(1 + 0.9*6.265 - 6.265)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.303 → 6.340\n",
      "    Formula: 6.303 + 0.1*(1 + 0.9*6.303 - 6.303)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.340 → 6.376\n",
      "    Formula: 6.340 + 0.1*(1 + 0.9*6.340 - 6.340)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.376 → 6.413\n",
      "    Formula: 6.376 + 0.1*(1 + 0.9*6.376 - 6.376)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 95.0 → 100.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.964 → 1.894\n",
      "    Formula: 1.964 + 0.1*(1 + 0.9*0.297 - 1.964)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.297 → 0.394\n",
      "    Formula: 0.297 + 0.1*(1 + 0.9*0.297 - 0.297)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.394 → 0.490\n",
      "    Formula: 0.394 + 0.1*(1 + 0.9*0.394 - 0.394)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.490 → 0.585\n",
      "    Formula: 0.490 + 0.1*(1 + 0.9*0.490 - 0.490)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(10), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [100.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(10), np.int64(2))\n",
      "    Q-update: Q[(np.int64(10), np.int64(2))][0]: 0.585 → 0.679\n",
      "    Formula: 0.585 + 0.1*(1 + 0.9*0.585 - 0.585)\n",
      "    Step summary: State [100.0, 20.0], Step reward: 1\n",
      "Episode 840: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 840 finished. Total reward: 20\n",
      "Final state: 100.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 850 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 40.0 → 45.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.494 → 9.545\n",
      "    Formula: 9.494 + 0.1*(1 + 0.9*10.000 - 9.494)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [45.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [45.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 45.0 → 50.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][1]: 9.545 → 9.344\n",
      "    Formula: 9.545 + 0.1*(1 + 0.9*7.265 - 9.545)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.265 → 7.292\n",
      "    Formula: 7.265 + 0.1*(1 + 0.9*7.265 - 7.265)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.292 → 7.320\n",
      "    Formula: 7.292 + 0.1*(1 + 0.9*7.292 - 7.292)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.320 → 7.346\n",
      "    Formula: 7.320 + 0.1*(1 + 0.9*7.320 - 7.320)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.346 → 7.373\n",
      "    Formula: 7.346 + 0.1*(1 + 0.9*7.346 - 7.346)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.373 → 7.399\n",
      "    Formula: 7.373 + 0.1*(1 + 0.9*7.373 - 7.373)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.399 → 7.425\n",
      "    Formula: 7.399 + 0.1*(1 + 0.9*7.399 - 7.399)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.425 → 7.451\n",
      "    Formula: 7.425 + 0.1*(1 + 0.9*7.425 - 7.425)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.451 → 7.476\n",
      "    Formula: 7.451 + 0.1*(1 + 0.9*7.451 - 7.451)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.476 → 7.502\n",
      "    Formula: 7.476 + 0.1*(1 + 0.9*7.476 - 7.476)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "Episode 850: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 850 finished. Total reward: 20\n",
      "Final state: 50.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 860 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -55.164 → -48.648\n",
      "    Formula: -55.164 + 0.1*(1 + 0.9*10.000 - -55.164)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.796 → 9.816\n",
      "    Formula: 9.796 + 0.1*(1 + 0.9*10.000 - 9.796)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "Episode 860: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 860 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 870 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "Episode 870: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 870 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.399 → 7.425\n",
      "    Formula: 7.399 + 0.1*(1 + 0.9*7.399 - 7.399)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.425 → 7.451\n",
      "    Formula: 7.425 + 0.1*(1 + 0.9*7.425 - 7.425)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.451 → 7.476\n",
      "    Formula: 7.451 + 0.1*(1 + 0.9*7.451 - 7.451)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(5), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [50.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(5), np.int64(1))\n",
      "    Q-update: Q[(np.int64(5), np.int64(1))][0]: 7.476 → 7.502\n",
      "    Formula: 7.476 + 0.1*(1 + 0.9*7.476 - 7.476)\n",
      "    Step summary: State [50.0, 9.0], Step reward: 1\n",
      "Episode 850: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 850 finished. Total reward: 20\n",
      "Final state: 50.0 prey, 9.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 860 ---\n",
      "Starting state: 20.0 prey, 5.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 5.0 → 3.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -55.164 → -48.648\n",
      "    Formula: -55.164 + 0.1*(1 + 0.9*10.000 - -55.164)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 20.0 → 25.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][1]: 9.796 → 9.816\n",
      "    Formula: 9.796 + 0.1*(1 + 0.9*10.000 - 9.796)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [25.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [25.0, 3.0], Step reward: 1\n",
      "Episode 860: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 860 finished. Total reward: 20\n",
      "Final state: 25.0 prey, 3.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 870 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "Episode 870: Reward = 20, Non-zero Q-values = 36\n",
      "\n",
      "Episode 870 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 15.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 880 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -57.810 → -61.129\n",
      "    Formula: -57.810 + 0.1*(-100 + 0.9*10.000 - -57.810)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 880: Reward = -91, Non-zero Q-values = 37\n",
      "\n",
      "Episode 880 finished. Total reward: -91\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 890 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 890: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 890 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 900 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.097 → 9.187\n",
      "    Formula: 9.097 + 0.1*(1 + 0.9*10.000 - 9.097)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "--- EPISODE 880 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -57.810 → -61.129\n",
      "    Formula: -57.810 + 0.1*(-100 + 0.9*10.000 - -57.810)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 880: Reward = -91, Non-zero Q-values = 37\n",
      "\n",
      "Episode 880 finished. Total reward: -91\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 890 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 13.0], Step reward: 1\n",
      "Episode 890: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 890 finished. Total reward: 20\n",
      "Final state: 60.0 prey, 13.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 900 ---\n",
      "Starting state: 60.0 prey, 15.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [60.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [60.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 60.0 → 65.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][1]: 9.097 → 9.187\n",
      "    Formula: 9.097 + 0.1*(1 + 0.9*10.000 - 9.097)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "Episode 900: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 900 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 910 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.599 → 9.639\n",
      "    Formula: 9.599 + 0.1*(1 + 0.9*10.000 - 9.599)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.639 → 9.465\n",
      "    Formula: 9.639 + 0.1*(1 + 0.9*7.671 - 9.639)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 7.671 → 7.695\n",
      "    Formula: 7.671 + 0.1*(1 + 0.9*7.671 - 7.671)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 7.695 → 7.718\n",
      "    Formula: 7.695 + 0.1*(1 + 0.9*7.695 - 7.695)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "Episode 910: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 910 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 920 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 15.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 15.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 15.0 → 13.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [65.0, 13.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [65.0, 13.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(6), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 13.0 → 11.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(6), np.int64(1))\n",
      "    Q-update: Q[(np.int64(6), np.int64(1))][2]: 9.999 → 9.999\n",
      "    Formula: 9.999 + 0.1*(1 + 0.9*10.000 - 9.999)\n",
      "    Step summary: State [65.0, 11.0], Step reward: 1\n",
      "Episode 900: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 900 finished. Total reward: 20\n",
      "Final state: 65.0 prey, 11.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 910 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.599 → 9.639\n",
      "    Formula: 9.599 + 0.1*(1 + 0.9*10.000 - 9.599)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 7.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 7.0 → 5.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.639 → 9.465\n",
      "    Formula: 9.639 + 0.1*(1 + 0.9*7.671 - 9.639)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 7.671 → 7.695\n",
      "    Formula: 7.671 + 0.1*(1 + 0.9*7.671 - 7.671)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 5.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(0))\n",
      "    Q-update: Q[(np.int64(4), np.int64(0))][0]: 7.695 → 7.718\n",
      "    Formula: 7.695 + 0.1*(1 + 0.9*7.695 - 7.695)\n",
      "    Step summary: State [40.0, 5.0], Step reward: 1\n",
      "Episode 910: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 910 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 5.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 920 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.143 → 9.229\n",
      "    Formula: 9.143 + 0.1*(1 + 0.9*10.000 - 9.143)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "Episode 920: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 920 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 930 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.229 → 9.306\n",
      "    Formula: 9.229 + 0.1*(1 + 0.9*10.000 - 9.229)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.306 → 9.095\n",
      "    Formula: 9.306 + 0.1*(1 + 0.9*6.883 - 9.306)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.883 → 6.915\n",
      "    Formula: 6.883 + 0.1*(1 + 0.9*6.883 - 6.883)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.915 → 6.945\n",
      "    Formula: 6.915 + 0.1*(1 + 0.9*6.915 - 6.915)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.945 → 6.976\n",
      "    Formula: 6.945 + 0.1*(1 + 0.9*6.945 - 6.945)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.976 → 7.006\n",
      "    Formula: 6.976 + 0.1*(1 + 0.9*6.976 - 6.976)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.006 → 7.036\n",
      "    Formula: 7.006 + 0.1*(1 + 0.9*7.006 - 7.006)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.036 → 7.066\n",
      "    Formula: 7.036 + 0.1*(1 + 0.9*7.036 - 7.036)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 90.0 → 95.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.894 → 2.441\n",
      "    Formula: 1.894 + 0.1*(1 + 0.9*7.066 - 1.894)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.066 → 7.095\n",
      "    Formula: 7.066 + 0.1*(1 + 0.9*7.066 - 7.066)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.095 → 7.124\n",
      "    Formula: 7.095 + 0.1*(1 + 0.9*7.095 - 7.095)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.124 → 7.153\n",
      "    Formula: 7.124 + 0.1*(1 + 0.9*7.124 - 7.124)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.153 → 7.181\n",
      "    Formula: 7.153 + 0.1*(1 + 0.9*7.153 - 7.153)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.181 → 7.210\n",
      "    Formula: 7.181 + 0.1*(1 + 0.9*7.181 - 7.181)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.210 → 7.237\n",
      "    Formula: 7.210 + 0.1*(1 + 0.9*7.210 - 7.210)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "Episode 930: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 930 finished. Total reward: 20\n",
      "Final state: 95.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 940 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.143 → 9.229\n",
      "    Formula: 9.143 + 0.1*(1 + 0.9*10.000 - 9.143)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "Episode 920: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 920 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 930 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.229 → 9.306\n",
      "    Formula: 9.229 + 0.1*(1 + 0.9*10.000 - 9.229)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 85.0 → 90.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.306 → 9.095\n",
      "    Formula: 9.306 + 0.1*(1 + 0.9*6.883 - 9.306)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.883 → 6.915\n",
      "    Formula: 6.883 + 0.1*(1 + 0.9*6.883 - 6.883)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.915 → 6.945\n",
      "    Formula: 6.915 + 0.1*(1 + 0.9*6.915 - 6.915)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.945 → 6.976\n",
      "    Formula: 6.945 + 0.1*(1 + 0.9*6.945 - 6.945)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 6.976 → 7.006\n",
      "    Formula: 6.976 + 0.1*(1 + 0.9*6.976 - 6.976)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.006 → 7.036\n",
      "    Formula: 7.006 + 0.1*(1 + 0.9*7.006 - 7.006)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [90.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.036 → 7.066\n",
      "    Formula: 7.036 + 0.1*(1 + 0.9*7.036 - 7.036)\n",
      "    Step summary: State [90.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 90.0 → 95.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][1]: 1.894 → 2.441\n",
      "    Formula: 1.894 + 0.1*(1 + 0.9*7.066 - 1.894)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.066 → 7.095\n",
      "    Formula: 7.066 + 0.1*(1 + 0.9*7.066 - 7.066)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.095 → 7.124\n",
      "    Formula: 7.095 + 0.1*(1 + 0.9*7.095 - 7.095)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.124 → 7.153\n",
      "    Formula: 7.124 + 0.1*(1 + 0.9*7.124 - 7.124)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.153 → 7.181\n",
      "    Formula: 7.153 + 0.1*(1 + 0.9*7.153 - 7.153)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.181 → 7.210\n",
      "    Formula: 7.181 + 0.1*(1 + 0.9*7.181 - 7.181)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(9), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [95.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(9), np.int64(2))\n",
      "    Q-update: Q[(np.int64(9), np.int64(2))][0]: 7.210 → 7.237\n",
      "    Formula: 7.210 + 0.1*(1 + 0.9*7.210 - 7.210)\n",
      "    Step summary: State [95.0, 20.0], Step reward: 1\n",
      "Episode 930: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 930 finished. Total reward: 20\n",
      "Final state: 95.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 940 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -62.975 → -65.777\n",
      "    Formula: -62.975 + 0.1*(-100 + 0.9*10.000 - -62.975)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 940: Reward = -85, Non-zero Q-values = 37\n",
      "\n",
      "Episode 940 finished. Total reward: -85\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 950 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][2]: -72.600 → -74.440\n",
      "    Formula: -72.600 + 0.1*(-100 + 0.9*10.000 - -72.600)\n",
      "    Step summary: State [10.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 950: Reward = -100, Non-zero Q-values = 37\n",
      "\n",
      "Episode 950 finished. Total reward: -100\n",
      "Final state: 10.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 960 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -56.170 → -59.653\n",
      "    Formula: -56.170 + 0.1*(-100 + 0.9*10.000 - -56.170)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 960: Reward = -87, Non-zero Q-values = 37\n",
      "\n",
      "Episode 960 finished. Total reward: -87\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 970 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -62.975 → -65.777\n",
      "    Formula: -62.975 + 0.1*(-100 + 0.9*10.000 - -62.975)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 940: Reward = -85, Non-zero Q-values = 37\n",
      "\n",
      "Episode 940 finished. Total reward: -85\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 950 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][2]: -72.600 → -74.440\n",
      "    Formula: -72.600 + 0.1*(-100 + 0.9*10.000 - -72.600)\n",
      "    Step summary: State [10.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 950: Reward = -100, Non-zero Q-values = 37\n",
      "\n",
      "Episode 950 finished. Total reward: -100\n",
      "Final state: 10.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 960 ---\n",
      "Starting state: 10.0 prey, 3.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 10.0 → 15.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(1), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [15.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(1), np.int64(0))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 15.0 → 20.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(1), np.int64(0))][1]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [20.0, 3.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [20.0, 3.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(2), np.int64(0))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 3.0 → 1.0 predators\n",
      "    Reward: -100\n",
      "    New state indices: (np.int64(2), np.int64(0))\n",
      "    Q-update: Q[(np.int64(2), np.int64(0))][2]: -56.170 → -59.653\n",
      "    Formula: -56.170 + 0.1*(-100 + 0.9*10.000 - -56.170)\n",
      "    Step summary: State [20.0, 1.0], Step reward: -100\n",
      "    EXTINCTION! Ending episode early.\n",
      "Episode 960: Reward = -87, Non-zero Q-values = 37\n",
      "\n",
      "Episode 960 finished. Total reward: -87\n",
      "Final state: 20.0 prey, 1.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 970 ---\n",
      "Starting state: 40.0 prey, 9.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.716 → 9.744\n",
      "    Formula: 9.716 + 0.1*(1 + 0.9*10.000 - 9.716)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 970: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 970 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 980 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 980: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 980 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 990 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.204 → 9.284\n",
      "    Formula: 9.204 + 0.1*(1 + 0.9*10.000 - 9.204)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.974 → 9.977\n",
      "    Formula: 9.974 + 0.1*(1 + 0.9*10.000 - 9.974)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 990: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 990 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [40.0, 9.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [40.0, 9.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(4), np.int64(1))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 9.0 → 7.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(4), np.int64(1))\n",
      "    Q-update: Q[(np.int64(4), np.int64(1))][2]: 9.716 → 9.744\n",
      "    Formula: 9.716 + 0.1*(1 + 0.9*10.000 - 9.716)\n",
      "    Step summary: State [40.0, 7.0], Step reward: 1\n",
      "Episode 970: Reward = 20, Non-zero Q-values = 37\n",
      "\n",
      "Episode 970 finished. Total reward: 20\n",
      "Final state: 40.0 prey, 7.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 980 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 980: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 980 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 990 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 1 (add_prey)\n",
      "Printing action index: 1\n",
      "    Action: Add prey - 80.0 → 85.0 prey\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][1]: 9.204 → 9.284\n",
      "    Formula: 9.204 + 0.1*(1 + 0.9*10.000 - 9.204)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [85.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [85.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 2 (remove_predators)\n",
      "Printing action index: 2\n",
      "    Action: Remove predators - 20.0 → 18.0 predators\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][2]: 9.974 → 9.977\n",
      "    Formula: 9.974 + 0.1*(1 + 0.9*10.000 - 9.974)\n",
      "    Step summary: State [85.0, 18.0], Step reward: 1\n",
      "Episode 990: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 990 finished. Total reward: 20\n",
      "Final state: 85.0 prey, 18.0 predators\n",
      "--------------------------------------------------\n",
      "--- EPISODE 1000 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 1000: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 1000 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "\n",
      "=== TRAINING COMPLETE ===\n",
      "Final episode rewards: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "Average reward (all episodes): 4.6\n",
      "Average reward (last 20 episodes): 20.0\n",
      "Learning progress (non-zero Q-values): [np.int64(11), np.int64(17), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(25), np.int64(26), np.int64(28), np.int64(28), np.int64(29), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(38), np.int64(38), np.int64(38)]\n",
      "--- EPISODE 1000 ---\n",
      "Starting state: 80.0 prey, 20.0 predators\n",
      "\n",
      "  Step 1:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 2:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 3:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 4:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 5:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 6:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 7:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 8:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 9:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 10:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 11:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 12:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 13:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 14:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 15:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 16:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 17:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 18:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 19:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "\n",
      "  Step 20:\n",
      "    Current state indices: (np.int64(8), np.int64(2))\n",
      "    Chosen action: 0 (do_nothing)\n",
      "Printing action index: 0\n",
      "    Action: Do nothing - populations stay at [80.0, 20.0]\n",
      "    Reward: 1\n",
      "    New state indices: (np.int64(8), np.int64(2))\n",
      "    Q-update: Q[(np.int64(8), np.int64(2))][0]: 10.000 → 10.000\n",
      "    Formula: 10.000 + 0.1*(1 + 0.9*10.000 - 10.000)\n",
      "    Step summary: State [80.0, 20.0], Step reward: 1\n",
      "Episode 1000: Reward = 20, Non-zero Q-values = 38\n",
      "\n",
      "Episode 1000 finished. Total reward: 20\n",
      "Final state: 80.0 prey, 20.0 predators\n",
      "--------------------------------------------------\n",
      "\n",
      "=== TRAINING COMPLETE ===\n",
      "Final episode rewards: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "Average reward (all episodes): 4.6\n",
      "Average reward (last 20 episodes): 20.0\n",
      "Learning progress (non-zero Q-values): [np.int64(11), np.int64(17), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(25), np.int64(26), np.int64(28), np.int64(28), np.int64(29), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(31), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(32), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(33), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(34), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(35), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(36), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(37), np.int64(38), np.int64(38), np.int64(38)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== STARTING RL TRAINING ===\")\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Discount factor: {discount_factor}\")\n",
    "print(f\"  Epsilon (exploration): {epsilon}\")\n",
    "print()\n",
    "\n",
    "# Training settings\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 20  # Longer episodes for more learning opportunities\n",
    "\n",
    "# Diverse starting states for better exploration\n",
    "starting_states = [\n",
    "    [20, 5],   # Low populations - crisis scenario\n",
    "    [40, 9],   # Medium populations - original state\n",
    "    [60, 15],  # High populations - abundance scenario\n",
    "    [10, 3],   # Very low populations - near extinction\n",
    "    [80, 20]   # Very high populations - overpopulation\n",
    "]\n",
    "\n",
    "total_rewards = []  # Track performance\n",
    "learning_progress = []  # Track Q-values over time to see learning\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Show progress every 10 episodes\n",
    "    show_details = (episode + 1) % 10 == 0 or episode < 5\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"--- EPISODE {episode + 1} ---\")\n",
    "    \n",
    "    # Reset environment for new episode - random starting state\n",
    "    current_state = random.choice(starting_states).copy()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"Starting state: {current_state[0]:.1f} prey, {current_state[1]:.1f} predators\")\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if show_details:\n",
    "            print(f\"\\n  Step {step + 1}:\")\n",
    "        \n",
    "        # 1. Get current state indices\n",
    "        state_indices = get_state_indices(current_state[0], current_state[1])\n",
    "        if show_details:\n",
    "            print(f\"    Current state indices: {state_indices}\")\n",
    "        \n",
    "        # 2. Choose action\n",
    "        action = choose_action(state_indices[0], state_indices[1], epsilon)\n",
    "        action_name = actions[action]\n",
    "        if show_details:\n",
    "            print(f\"    Chosen action: {action} ({action_name})\")\n",
    "        \n",
    "        # 3. Apply action\n",
    "        new_state = apply_action(current_state, action, verbose=show_details)\n",
    "        \n",
    "        # 4. Calculate reward\n",
    "        reward = calculate_reward(new_state[0], new_state[1])\n",
    "        if show_details:\n",
    "            print(f\"    Reward: {reward}\")\n",
    "        \n",
    "        # 5. Get new state indices\n",
    "        new_state_indices = get_state_indices(new_state[0], new_state[1])\n",
    "        if show_details:\n",
    "            print(f\"    New state indices: {new_state_indices}\")\n",
    "        \n",
    "        # 6. Update Q-table\n",
    "        update_q_table(state_indices, action, reward, new_state_indices, learning_rate, discount_factor, verbose=show_details)\n",
    "        \n",
    "        # 7. Update for next iteration\n",
    "        current_state = new_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if show_details:\n",
    "            print(f\"    Step summary: State [{current_state[0]:.1f}, {current_state[1]:.1f}], Step reward: {reward}\")\n",
    "        \n",
    "        # Stop episode early if extinction occurs\n",
    "        if reward == -100:\n",
    "            if show_details:\n",
    "                print(f\"    EXTINCTION! Ending episode early.\")\n",
    "            break\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "    \n",
    "    # Track learning progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        non_zero_q = np.count_nonzero(q_table)\n",
    "        learning_progress.append(non_zero_q)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}, Non-zero Q-values = {non_zero_q}\")\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"\\nEpisode {episode + 1} finished. Total reward: {episode_reward}\")\n",
    "        print(f\"Final state: {current_state[0]:.1f} prey, {current_state[1]:.1f} predators\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n=== TRAINING COMPLETE ===\")\n",
    "print(f\"Final episode rewards: {total_rewards[-10:]}\")\n",
    "print(f\"Average reward (all episodes): {np.mean(total_rewards):.1f}\")\n",
    "print(f\"Average reward (last 20 episodes): {np.mean(total_rewards[-20:]):.1f}\")\n",
    "print(f\"Learning progress (non-zero Q-values): {learning_progress}\")\n",
    "\n",
    "# Plot learning progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Episode Rewards Over Time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "episode_markers = list(range(10, num_episodes + 1, 10))\n",
    "plt.plot(episode_markers, learning_progress, 'o-')\n",
    "plt.title('Learning Progress (Non-zero Q-values)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of Non-zero Q-values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f636b",
   "metadata": {},
   "source": [
    "## 11. Q-Table Analysis and Learned Policy Inspection\n",
    "\n",
    "This code examines what the reinforcement learning agent has learned by inspecting its Q-table after training. It analyzes the agent's learned policy by checking Q-values for several representative ecosystem states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1d837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q-TABLE INSPECTION ===\n",
      "\n",
      "State: 40 prey, 9 predators (indices: 4, 1)\n",
      "Q-values: [10.          9.44557698  9.58742046]\n",
      "Best action: 0 (do_nothing) with Q-value: 10.000\n",
      "  do_nothing: 10.000 ←BEST\n",
      "  add_prey: 9.446\n",
      "  remove_predators: 9.587\n",
      "\n",
      "State: 20 prey, 5 predators (indices: 2, 0)\n",
      "Q-values: [ 10.           9.75712006 -55.50911768]\n",
      "Best action: 0 (do_nothing) with Q-value: 10.000\n",
      "  do_nothing: 10.000 ←BEST\n",
      "  add_prey: 9.757\n",
      "  remove_predators: -55.509\n",
      "\n",
      "State: 80 prey, 20 predators (indices: 8, 2)\n",
      "Q-values: [10.          9.35538515  9.9845986 ]\n",
      "Best action: 0 (do_nothing) with Q-value: 10.000\n",
      "  do_nothing: 10.000 ←BEST\n",
      "  add_prey: 9.355\n",
      "  remove_predators: 9.985\n",
      "\n",
      "State: 10 prey, 3 predators (indices: 1, 0)\n",
      "Q-values: [  8.27686597  10.         -76.09595422]\n",
      "Best action: 1 (add_prey) with Q-value: 10.000\n",
      "  do_nothing: 8.277\n",
      "  add_prey: 10.000 ←BEST\n",
      "  remove_predators: -76.096\n",
      "\n",
      "State: 60 prey, 15 predators (indices: 6, 1)\n",
      "Q-values: [10.          9.23809707  9.99990254]\n",
      "Best action: 0 (do_nothing) with Q-value: 10.000\n",
      "  do_nothing: 10.000 ←BEST\n",
      "  add_prey: 9.238\n",
      "  remove_predators: 10.000\n",
      "\n",
      "=== SUMMARY ===\n",
      "Non-zero Q-values in table: 38\n",
      "Min Q-value: -76.096\n",
      "Max Q-value: 10.000\n",
      "Mean Q-value: 0.039\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the Q-table to see what the agent learned\n",
    "print(\"=== Q-TABLE INSPECTION ===\")\n",
    "\n",
    "# Check a few key states to see what the agent learned\n",
    "test_states = [\n",
    "    (40, 9),   # Initial state\n",
    "    (20, 5),   # Lower populations\n",
    "    (80, 20),  # Higher populations\n",
    "    (10, 3),   # Risky state\n",
    "    (60, 15)   # Abundant state\n",
    "]\n",
    "\n",
    "for prey_pop, pred_pop in test_states:\n",
    "    prey_idx, pred_idx = get_state_indices(prey_pop, pred_pop)\n",
    "    q_values = q_table[prey_idx, pred_idx, :]\n",
    "    \n",
    "    print(f\"\\nState: {prey_pop} prey, {pred_pop} predators (indices: {prey_idx}, {pred_idx})\")\n",
    "    print(f\"Q-values: {q_values}\")\n",
    "    \n",
    "    # Find best action\n",
    "    best_action = np.argmax(q_values)\n",
    "    best_q_value = q_values[best_action]\n",
    "    \n",
    "    print(f\"Best action: {best_action} ({actions[best_action]}) with Q-value: {best_q_value:.3f}\")\n",
    "    \n",
    "    # Show all actions\n",
    "    for i, (action_name, q_val) in enumerate(zip(actions, q_values)):\n",
    "        marker = \" ←BEST\" if i == best_action else \"\"\n",
    "        print(f\"  {action_name}: {q_val:.3f}{marker}\")\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Non-zero Q-values in table: {np.count_nonzero(q_table)}\")\n",
    "print(f\"Min Q-value: {np.min(q_table):.3f}\")\n",
    "print(f\"Max Q-value: {np.max(q_table):.3f}\")\n",
    "print(f\"Mean Q-value: {np.mean(q_table):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2fe624",
   "metadata": {},
   "source": [
    "## 12. Agent Performance Comparison Study\n",
    "\n",
    "Comparing RL agent performance against two baseline approaches:\n",
    "\n",
    "1. **No Intervention**: Natural Lotka-Volterra dynamics (baseline)\n",
    "2. **Random Agent**: Takes random actions at each step \n",
    "3. **Trained RL Agent**: Uses the learned Q-table to make smart decisions\n",
    "\n",
    "This comparison will demonstrate the value of reinforcement learning for ecosystem management by showing how the trained agent performs compared to random actions and natural evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca1507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario simulation function defined!\n",
      "Ready to test 4 scenarios with 30 steps each\n"
     ]
    }
   ],
   "source": [
    "def simulate_scenario(scenario_type, initial_state, num_steps, verbose=False):\n",
    "    \"\"\"\n",
    "    Simulate ecosystem management under different approaches.\n",
    "    \n",
    "    This revised function now correctly simulates one time step of Lotka-Volterra\n",
    "    dynamics after each action, creating a more realistic test.\n",
    "    \"\"\"\n",
    "    trajectory = [initial_state.copy()]\n",
    "    current_state = initial_state.copy()\n",
    "    total_reward = 0\n",
    "    survival_steps = 0\n",
    "    \n",
    "    # Define a short time interval for one step of simulation\n",
    "    t_step = [0, 1] \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n=== {scenario_type.upper().replace('_', ' ')} SIMULATION ===\")\n",
    "        print(f\"Starting state: {current_state[0]:.1f} prey, {current_state[1]:.1f} predators\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # 1. CHOOSE ACTION based on scenario\n",
    "        if scenario_type == 'no_intervention':\n",
    "            action = 0  # Always do nothing\n",
    "        elif scenario_type == 'random_agent':\n",
    "            action = random.randint(0, num_actions - 1)\n",
    "        elif scenario_type == 'rl_agent':\n",
    "            state_indices = get_state_indices(current_state[0], current_state[1])\n",
    "            q_values = q_table[state_indices[0], state_indices[1], :]\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        # 2. APPLY ACTION to get an intermediate state\n",
    "        interim_state = apply_action(current_state, action, verbose=False)\n",
    "        \n",
    "        # 3. SIMULATE one time step of natural dynamics\n",
    "        solution = odeint(lotka_volterra, interim_state, t_step, args=(\n",
    "            parameters['alpha'], parameters['beta'], \n",
    "            parameters['delta'], parameters['gamma']\n",
    "        ))\n",
    "        new_state = solution[-1] # The state after one time step\n",
    "        \n",
    "        # Ensuring populations don't go negative from simulation\n",
    "        new_state[0] = max(0, new_state[0])\n",
    "        new_state[1] = max(0, new_state[1])\n",
    "\n",
    "        # 4. CALCULATE REWARD and update trajectory\n",
    "        reward = calculate_reward(new_state[0], new_state[1])\n",
    "        total_reward += reward\n",
    "        \n",
    "        current_state = new_state\n",
    "        trajectory.append(current_state.copy())\n",
    "        \n",
    "        if verbose:\n",
    "            action_name = actions[action]\n",
    "            print(f\"Step {step+1}: Action={action_name}, State=[{current_state[0]:.1f}, {current_state[1]:.1f}], Reward={reward}\")\n",
    "        \n",
    "        # 5. CHECK FOR EXTINCTION\n",
    "        if reward == -100:\n",
    "            survival_steps = step + 1\n",
    "            if verbose:\n",
    "                print(f\"EXTINCTION at step {step+1}!\")\n",
    "            break\n",
    "        else:\n",
    "            survival_steps = step + 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final result: Total reward = {total_reward}, Survived {survival_steps}/{num_steps} steps\")\n",
    "    \n",
    "    return trajectory, total_reward, survival_steps\n",
    "\n",
    "print(\"Scenario simulation function defined!\")\n",
    "\n",
    "test_scenarios = [\n",
    "    [20, 5],   # Crisis scenario\n",
    "    [40, 9],   # Medium populations  \n",
    "    [60, 15],  # High populations\n",
    "    [10, 3],   # Near extinction\n",
    "]\n",
    "\n",
    "num_test_steps = 30  # Longer test to see differences\n",
    "\n",
    "print(f\"Ready to test {len(test_scenarios)} scenarios with {num_test_steps} steps each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE AGENT COMPARISON ===\n",
      "\n",
      "--- SCENARIO 1: Starting with 20 prey, 5 predators ---\n",
      "  No Intervention: Reward=-81, Survived=20/30, Final=[4.9, 7.7]\n",
      "  Random Agent: Reward=-99, Survived= 2/30, Final=[22.3, 1.5]\n",
      "  Rl Agent: Reward=-81, Survived=20/30, Final=[4.9, 7.7]\n",
      "\n",
      "--- SCENARIO 2: Starting with 40 prey, 9 predators ---\n",
      "  No Intervention: Reward=-92, Survived= 9/30, Final=[3.8, 19.1]\n",
      "  Random Agent: Reward=-91, Survived=10/30, Final=[3.9, 20.2]\n",
      "  Rl Agent: Reward=-92, Survived= 9/30, Final=[3.8, 19.1]\n",
      "\n",
      "--- SCENARIO 3: Starting with 60 prey, 15 predators ---\n",
      "  No Intervention: Reward=-95, Survived= 6/30, Final=[2.9, 32.2]\n",
      "  Random Agent: Reward=-95, Survived= 6/30, Final=[3.3, 29.1]\n",
      "  Rl Agent: Reward=-95, Survived= 6/30, Final=[3.5, 31.1]\n",
      "\n",
      "--- SCENARIO 4: Starting with 10 prey, 3 predators ---\n",
      "  No Intervention: Reward= 30, Survived=30/30, Final=[9.4, 7.7]\n",
      "  Random Agent: Reward=-97, Survived= 4/30, Final=[17.7, 1.3]\n",
      "  Rl Agent: Reward= 30, Survived=30/30, Final=[5.3, 6.6]\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Average Performance Across 4 Test Scenarios:\n",
      "  No Intervention: Avg Reward =  -59.5, Avg Survival = 16.2/30, Extinctions = 3/4\n",
      "  Random Agent   : Avg Reward =  -95.5, Avg Survival =  5.5/30, Extinctions = 4/4\n",
      "  Rl Agent       : Avg Reward =  -59.5, Avg Survival = 16.2/30, Extinctions = 3/4\n",
      "\n",
      "Comparison completed! Results ready for visualization.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== COMPREHENSIVE AGENT COMPARISON ===\")\n",
    "\n",
    "comparison_results = []\n",
    "scenario_types = ['no_intervention', 'random_agent', 'rl_agent']\n",
    "\n",
    "for i, initial_state in enumerate(test_scenarios):\n",
    "    print(f\"\\n--- SCENARIO {i+1}: Starting with {initial_state[0]} prey, {initial_state[1]} predators ---\")\n",
    "    \n",
    "    scenario_results = {}\n",
    "    \n",
    "    for scenario_type in scenario_types:\n",
    "        trajectory, total_reward, survival_steps = simulate_scenario(\n",
    "            scenario_type, initial_state, num_test_steps, verbose=False\n",
    "        )\n",
    "        \n",
    "        scenario_results[scenario_type] = {\n",
    "            'trajectory': trajectory,\n",
    "            'total_reward': total_reward,\n",
    "            'survival_steps': survival_steps,\n",
    "            'final_state': trajectory[-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"  {scenario_type.replace('_', ' ').title()}: \"\n",
    "              f\"Reward={total_reward:3d}, Survived={survival_steps:2d}/{num_test_steps}, \"\n",
    "              f\"Final=[{trajectory[-1][0]:.1f}, {trajectory[-1][1]:.1f}]\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'initial_state': initial_state,\n",
    "        'results': scenario_results\n",
    "    })\n",
    "\n",
    "print(f\"\\n=== SUMMARY STATISTICS ===\")\n",
    "\n",
    "# Averages across all scenarios\n",
    "avg_rewards = {scenario_type: 0 for scenario_type in scenario_types}\n",
    "avg_survival = {scenario_type: 0 for scenario_type in scenario_types}\n",
    "extinction_count = {scenario_type: 0 for scenario_type in scenario_types}\n",
    "\n",
    "for result in comparison_results:\n",
    "    for scenario_type in scenario_types:\n",
    "        avg_rewards[scenario_type] += result['results'][scenario_type]['total_reward']\n",
    "        avg_survival[scenario_type] += result['results'][scenario_type]['survival_steps']\n",
    "        if result['results'][scenario_type]['survival_steps'] < num_test_steps:\n",
    "            extinction_count[scenario_type] += 1\n",
    "\n",
    "num_scenarios = len(comparison_results)\n",
    "\n",
    "print(f\"Average Performance Across {num_scenarios} Test Scenarios:\")\n",
    "for scenario_type in scenario_types:\n",
    "    avg_reward = avg_rewards[scenario_type] / num_scenarios\n",
    "    avg_surv = avg_survival[scenario_type] / num_scenarios\n",
    "    extinctions = extinction_count[scenario_type]\n",
    "    \n",
    "    print(f\"  {scenario_type.replace('_', ' ').title():15}: \"\n",
    "          f\"Avg Reward = {avg_reward:6.1f}, \"\n",
    "          f\"Avg Survival = {avg_surv:4.1f}/{num_test_steps}, \"\n",
    "          f\"Extinctions = {extinctions}/{num_scenarios}\")\n",
    "\n",
    "print(f\"\\nComparison completed! Results ready for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449e18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED COMPARISON TABLE\n",
      "================================================================================\n",
      "\n",
      "Scenario 1 - Starting State: [20, 5]\n",
      "------------------------------------------------------------\n",
      "Approach        Total Reward Survival   Final State    \n",
      "------------------------------------------------------------\n",
      "No Intervention -81          20/30      [4.9, 7.7]     \n",
      "Random Agent    -99          2/30       [22.3, 1.5]    \n",
      "Trained RL Agent -81          20/30      [4.9, 7.7]     \n",
      "\n",
      "Scenario 2 - Starting State: [40, 9]\n",
      "------------------------------------------------------------\n",
      "Approach        Total Reward Survival   Final State    \n",
      "------------------------------------------------------------\n",
      "No Intervention -92          9/30       [3.8, 19.1]    \n",
      "Random Agent    -91          10/30      [3.9, 20.2]    \n",
      "Trained RL Agent -92          9/30       [3.8, 19.1]    \n",
      "\n",
      "Scenario 3 - Starting State: [60, 15]\n",
      "------------------------------------------------------------\n",
      "Approach        Total Reward Survival   Final State    \n",
      "------------------------------------------------------------\n",
      "No Intervention -95          6/30       [2.9, 32.2]    \n",
      "Random Agent    -95          6/30       [3.3, 29.1]    \n",
      "Trained RL Agent -95          6/30       [3.5, 31.1]    \n",
      "\n",
      "Scenario 4 - Starting State: [10, 3]\n",
      "------------------------------------------------------------\n",
      "Approach        Total Reward Survival   Final State    \n",
      "------------------------------------------------------------\n",
      "No Intervention 30           30/30      [9.4, 7.7]     \n",
      "Random Agent    -97          4/30       [17.7, 1.3]    \n",
      "Trained RL Agent 30           30/30      [5.3, 6.6]     \n",
      "\n",
      "================================================================================\n",
      "CONCLUSION: Does the RL agent outperform random and natural approaches?\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive visualization of comparison results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "colors = ['blue', 'orange', 'green']\n",
    "scenario_labels = ['No Intervention', 'Random Agent', 'Trained RL Agent']\n",
    "\n",
    "# Plot 1: Population trajectories for each test scenario\n",
    "for i, result in enumerate(comparison_results):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    if i < 4:  # 4 test scenarios\n",
    "        ax = axes[row, col] if row < 2 else axes[1, col]\n",
    "        \n",
    "        initial_state = result['initial_state']\n",
    "        ax.set_title(f'Scenario {i+1}: Start [{initial_state[0]}, {initial_state[1]}]')\n",
    "        \n",
    "        for j, scenario_type in enumerate(scenario_types):\n",
    "            trajectory = result['results'][scenario_type]['trajectory']\n",
    "            prey_pop = [state[0] for state in trajectory]\n",
    "            predator_pop = [state[1] for state in trajectory]\n",
    "            \n",
    "            ax.plot(prey_pop, label=f'{scenario_labels[j]} (Prey)', \n",
    "                   color=colors[j], linestyle='-', alpha=0.8)\n",
    "            ax.plot(predator_pop, label=f'{scenario_labels[j]} (Predator)', \n",
    "                   color=colors[j], linestyle='--', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Time Steps')\n",
    "        ax.set_ylabel('Population')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Performance comparison bar charts\n",
    "ax_rewards = axes[0, 2]\n",
    "ax_survival = axes[1, 2]\n",
    "\n",
    "# Rewards comparison\n",
    "reward_values = [avg_rewards[scenario_type] / num_scenarios for scenario_type in scenario_types]\n",
    "bars1 = ax_rewards.bar(scenario_labels, reward_values, color=colors, alpha=0.7)\n",
    "ax_rewards.set_title('Average Total Reward')\n",
    "ax_rewards.set_ylabel('Average Reward')\n",
    "ax_rewards.grid(True, alpha=0.3)\n",
    "\n",
    "# Value labels on bars\n",
    "for bar, value in zip(bars1, reward_values):\n",
    "    height = bar.get_height()\n",
    "    ax_rewards.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                   f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Survival comparison\n",
    "survival_values = [avg_survival[scenario_type] / num_scenarios for scenario_type in scenario_types]\n",
    "bars2 = ax_survival.bar(scenario_labels, survival_values, color=colors, alpha=0.7)\n",
    "ax_survival.set_title('Average Survival Steps')\n",
    "ax_survival.set_ylabel(f'Steps (out of {num_test_steps})')\n",
    "ax_survival.set_ylim(0, num_test_steps + 2)\n",
    "ax_survival.grid(True, alpha=0.3)\n",
    "\n",
    "# Value labels on bars\n",
    "for bar, value in zip(bars2, survival_values):\n",
    "    height = bar.get_height()\n",
    "    ax_survival.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    f'{value:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(comparison_results):\n",
    "    initial_state = result['initial_state']\n",
    "    print(f\"\\nScenario {i+1} - Starting State: [{initial_state[0]}, {initial_state[1]}]\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Approach':<15} {'Total Reward':<12} {'Survival':<10} {'Final State':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for scenario_type in scenario_types:\n",
    "        data = result['results'][scenario_type]\n",
    "        final_state = data['final_state']\n",
    "        survival_str = f\"{data['survival_steps']}/{num_test_steps}\"\n",
    "        final_state_str = f\"[{final_state[0]:.1f}, {final_state[1]:.1f}]\"\n",
    "        \n",
    "        print(f\"{scenario_labels[scenario_types.index(scenario_type)]:<15} \"\n",
    "              f\"{data['total_reward']:<12} \"\n",
    "              f\"{survival_str:<10} \"\n",
    "              f\"{final_state_str:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lotkaVolterra_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
